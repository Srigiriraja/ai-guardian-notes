Private Cloud : A cloud delivering of required servies over the internet which is very cost effective , having full access and control over that used by is single entity.
Public Cloud : A public cloud is built, controlled, and maintained by a third-party cloud provider. With a public cloud, anyone that wants to purchase cloud services can access and use resources. The general public availability is a key difference between public and private clouds.
Hybrid Cloud : Hybrid cloud allows us to keep sensitive or latency-critical workloads on-premises while leveraging Azure‚Äôs scalability and DevOps tools. At LTIMindtree, we managed AKS workloads on Azure while maintaining local Active Directory for compliance. Our pipelines deployed across both environments using a self-hosted agent connected via ExpressRoute.‚Äù
Scenario: A user logs in to access a Loan Processing App
Login Request ‚Üí Hits Azure AD (Cloud)

Azure AD checks user identity + MFA policy.

If approved, user accesses App hosted in Azure App Service

The App makes a secure call to On-Prem SQL Server (via Hybrid Connection or ExpressRoute)

The app logs and telemetry go to Azure Monitor and App Insights

If there's an outage, Azure Site Recovery spins up DR in Azure for the On-Prem SQL

 Hybrid Connectivity Options Used:
Tool	Purpose
VPN Gateway	Connect Azure and On-Prem securely
ExpressRoute	High-speed private connection
Azure Arc	Manage on-prem servers via Azure Portal
Azure Hybrid Identity	Azure AD Connect syncs on-prem AD with cloud

Real Example 2: LTIMindtree for Automotive Client
Use Case:
The client had a .NET Core microservices-based platform. They used:

Azure AKS to host microservices

Azure DevOps for CI/CD

On-prem Jenkins for legacy pipelines

Local Oracle DB and Active Directory

Azure Key Vault for secrets

Postman + Trivy integrated in CI

Pain Point:
They couldn't move Oracle DB due to vendor restrictions. So their services on AKS accessed on-prem DB over VPN.

How You Can Pitch It in Interview:
‚ÄúAt LTIMindtree, I worked on a hybrid cloud project where our AKS-hosted services in Azure connected to an on-prem Oracle DB using VPN Gateway. We built our CI/CD using Azure DevOps, deployed Terraform-based infrastructure, and integrated security scanning with Trivy and Checkov. Monitoring was unified via Azure Monitor and Grafana dashboards that collected logs from both cloud and on-prem VMs using Azure Arc.‚Äù

Summary of Hybrid Cloud Benefits
Benefit	Example
Compliance	Keeping PII data in on-prem SQL, rest in cloud
Cost Optimization	Run Dev/Test in cloud, Prod on-prem for licensing
Modernization Flexibility	Slowly move apps to AKS without total migration
Disaster Recovery	Azure as DR for on-prem servers
Unified Monitoring	Azure Monitor + Log Analytics + Arc across environments

With a traditional datacenter, you try to estimate the future resource needs. If you overestimate, you spend more on your datacenter than you need to and potentially waste money. If you underestimate, your datacenter will quickly reach capacity and your applications and services may suffer from decreased performance. Fixing an under-provisioned datacenter can take a long time. You may need to order, receive, and install more hardware. You'll also need to add power, cooling, and networking for the extra hardware.

In a cloud-based model, you don‚Äôt have to worry about getting the resource needs just right. If you find that you need more virtual machines, you add more. If the demand drops and you don‚Äôt need as many virtual machines, you remove machines as needed. Either way, you‚Äôre only paying for the virtual machines that you use, not the ‚Äúextra capacity‚Äù that the cloud provider has on hand.

Management of the cloud
Management of the cloud speaks to managing your cloud resources. In the cloud, you can:

Automatically scale resource deployment based on need.
Deploy resources based on a preconfigured template, removing the need for manual configuration.
Monitor the health of resources and automatically replace failing resources.
Receive automatic alerts based on configured metrics, so you‚Äôre aware of performance in real time.

Regions
A region is a geographical area on the planet that contains at least one, but potentially multiple datacenters that are nearby and networked together with a low-latency network. Azure intelligently assigns and controls the resources within each region to ensure workloads are appropriately balanced.

When you deploy a resource in Azure, you'll often need to choose the region where you want your resource deployed.

Azure Back up and DR 
üß† Real-World Example (First American India)
AKS-based .NET App stored data in Azure SQL

Azure SQL DB had long-term retention (LTR) via RSV

App VMs had daily snapshot backup policies

Recovery Vault named: rsv-prod-india

Located in Central India and used GZRS storage

Access was restricted using RBAC roles

üë®‚Äçüíª Terraform Snippet (Backup Vault Creation)
hcl
Copy
Edit
resource "azurerm_recovery_services_vault" "example" {
  name                = "rsv-prod"
  location            = "Central India"
  resource_group_name = "rg-prod-backup"
  sku                 = "Standard"
  soft_delete_enabled = true
}
üîÅ Scheduling and Retention
You define this inside Backup Policy

Daily/Weekly

Retain for 7 days, 30 days, 180 days, etc.

Instant restore or long-term backup (e.g., 1 year)

üßæ Summary
Task	Where to Do It
Create Backup Vault	Recovery Services Vault (RSV)
Store VM Backups	Inside RSV, with LRS/GRS/GZRS redundancy
Schedule & Retain Backups	Backup Policy in the same RSV
Restore Data	From RSV backup pane


Use availability zones in your apps
Zonal services: You pin the resource to a specific zone (for example, VMs, managed disks, IP addresses).
Zone-redundant services: The platform replicates automatically across zones (for example, zone-redundant storage, SQL Database).
Non-regional services: Services are always available from Azure geographies and are resilient to zone-wide outages as well as region-wide outages.

  Azure may be broken down into two main groupings: the physical infrastructure, and the management infrastructure.

Physical infrastructure
The physical infrastructure for Azure starts with datacenters. Conceptually, the datacenters are the same as large corporate datacenters. They‚Äôre facilities with resources arranged in racks, with dedicated power, cooling, and networking infrastructure.

As a global cloud provider, Azure has datacenters around the world. However, these individual datacenters aren‚Äôt directly accessible. Datacenters are grouped into Azure Regions or Azure Availability Zones that are designed to help you achieve resiliency and reliability for your business-critical workloads.

The Global infrastructure site gives you a chance to interactively explore the underlying Azure infrastructure.

Use availability zones in your apps
You want to ensure your services and data are redundant so you can protect your information in case of failure. When you host your infrastructure, setting up your own redundancy requires that you create duplicate hardware environments. Azure can help make your app highly available through availability zones.

You can use availability zones to run mission-critical applications and build high-availability into your application architecture by co-locating your compute, storage, networking, and data resources within an availability zone and replicating in other availability zones. Keep in mind that there could be a cost to duplicating your services and transferring data between availability zones.

Availability zones are primarily for VMs, managed disks, load balancers, and SQL databases. Azure services that support availability zones fall into three categories:

Zonal services: You pin the resource to a specific zone (for example, VMs, managed disks, IP addresses).
Zone-redundant services: The platform replicates automatically across zones (for example, zone-redundant storage, SQL Database).
Non-regional services: Services are always available from Azure geographies and are resilient to zone-wide outages as well as region-wide outages.
Even with the additional resiliency that availability zones provide, it‚Äôs possible that an event could be so large that it impacts multiple availability zones in a single region. To provide even further resilience, Azure has Region Pairs.
Region pairs
Most Azure regions are paired with another region within the same geography (such as US, Europe, or Asia) at least 300 miles away. This approach allows for the replication of resources across a geography that helps reduce the likelihood of interruptions because of events such as natural disasters, civil unrest, power outages, or physical network outages that affect an entire region. For example, if a region in a pair was affected by a natural disaster, services would automatically fail over to the other region in its region pair.
Not all Azure services automatically replicate data or automatically fall back from a failed region to cross-replicate to another enabled region. In these scenarios, recovery and replication must be configured by the customer.

 Important

Most regions are paired in two directions, meaning they are the backup for the region that provides a backup for them (West US and East US back each other up). However, some regions, such as West India and Brazil South, are paired in only one direction. In a one-direction pairing, the Primary region does not provide backup for its secondary region. So, even though West India‚Äôs secondary region is South India, South India does not rely on West India. West India's secondary region is South India, but South India's secondary region is Central India. Brazil South is unique because it's paired with a region outside of its geography. Brazil South's secondary region is South Central US. The secondary region of South Central US isn't Brazil South.

Describe Azure management infrastructure

The management infrastructure includes Azure resources and resource groups, subscriptions, and accounts. Understanding the hierarchical organization will help you plan your projects and products within Azure‚Äôs

Azure subscriptions : In azure Subscriptions we have two one is managenment level and another one 

Using Azure requires an Azure subscription. A subscription provides you with authenticated and authorized access to Azure products and services. It also allows you to provision resources. An Azure subscription links to an Azure account, which is an identity in Microsoft Entra ID or in a directory that Microsoft Entra ID trusts.

An account can have multiple subscriptions, but it‚Äôs only required to have one. In a multi-subscription account, you can use the subscriptions to configure different billing models and apply different access-management policies. You can use Azure subscriptions to define boundaries around Azure products, services, and resources. There are two types of subscription boundaries that you can use:

Billing boundary: This subscription type determines how an Azure account is billed for using Azure. You can create multiple subscriptions for different types of billing requirements. Azure generates separate billing reports and invoices for each subscription so that you can organize and manage costs.
Access control boundary: Azure applies access-management policies at the subscription level, and you can create separate subscriptions to reflect different organizational structures. An example is that within a business, you have different departments to which you apply distinct Azure subscription policies. This billing model allows you to manage and control access to the resources that users provision with specific subscriptions.

Access control boundary 

Users can only provision/manage services in subscriptions they have access to.

It helps companies segment risk, compliance, and cost centers.

Policies like ‚Äúonly allow VM sizes B1 or B2 in Dev subscription‚Äù can be applied easily.

Azure management groups

Imagine you‚Äôre at CDK Global managing 10 subscriptions for different clients and environments:

üí° Management Groups:

if you‚Äôre dealing with multiple applications, multiple development teams, in multiple geographies.

If you have many subscriptions, you might need a way to efficiently manage access, policies, and compliance for those subscriptions. Azure management groups provide a level of scope above subscriptions. You organize subscriptions into containers called management groups and apply governance conditions to the management groups. All subscriptions within a management group automatically inherit the conditions applied to the management group, the same way that resource groups inherit settings from subscriptions and resources inherit from resource groups. Management groups give you enterprise-grade management at a large scale, no matter what type of subscriptions you might have. Management groups can be nested.

management

MG-CDK-Prod ‚Üí contains all subscriptions for production

MG-CDK-Dev ‚Üí contains all dev/test subscriptions

You apply:

Security policies

Cost limits

Global RBAC roles

üìÇ Resource Groups:
Inside a subscription like CDK-Prod-Client1-Sub, you have:

rg-prod-client1-api ‚Üí contains AKS, ACR, App Gateway

rg-prod-client1-db ‚Üí contains Azure SQL, Key Vault

These are used for resource isolation, CI/CD deployment targets, or monitoring.

üß† Analogy:
Think of Management Groups like folders at the organization level,
and Resource Groups like folders inside a project.

üéØ Summary:
Statement	True or False
Management Groups = Resource Groups	‚ùå False
Management Groups group subscriptions	‚úÖ True
Resource Groups group Azure resources	‚úÖ True
You can deploy VMs in a Management Group	‚ùå False
You can apply RBAC at both levels	‚úÖ True

Some examples of how you could use management groups might be:

Create a hierarchy that applies a policy. You could limit VM locations to the US West Region in a group called Production. This policy will inherit onto all the subscriptions that are descendants of that management group and will apply to all VMs under those subscriptions. This security policy can't be altered by the resource or subscription owner, which allows for improved governance.
Provide user access to multiple subscriptions. By moving multiple subscriptions under a management group, you can create one Azure role-based access control (Azure RBAC) assignment on the management group. Assigning Azure RBAC at the management group level means that all sub-management groups, subscriptions, resource groups, and resources underneath that management group would also inherit those permissions. One assignment on the management group can enable users to have access to everything they need instead of scripting Azure RBAC over different subscriptions.


‚úÖ **Yes, you're absolutely right.**
In **Azure production-grade environments**, especially **enterprise-scale** setups, **policies should ideally be applied at the `Management Group` level** ‚Äî and here's exactly why:

---

### üéØ **Why Use Management Groups for Azure Policies?**

| üîπ Feature                 | üîπ Benefit                                                                                   |
| -------------------------- | -------------------------------------------------------------------------------------------- |
| **Policy Inheritance**     | All **subscriptions under the management group** inherit the policy automatically.           |
| **Centralized Governance** | Apply **security, tagging, location restrictions** centrally across multiple teams/projects. |
| **Scalability**            | One policy assignment covers hundreds of subscriptions‚Äî**zero redundancy**.                  |
| **Compliance**             | Ensures all environments (Dev/Test/Prod) follow **organization-wide compliance** rules.      |

---

### üèóÔ∏è Real-Time Production Scenario

**Company**: LTIMindtree or First American India
**Structure**:

```
Tenant (Directory)
‚îî‚îÄ‚îÄ Management Groups
    ‚îú‚îÄ‚îÄ Corp-IT
    ‚îÇ   ‚îú‚îÄ‚îÄ Sub-Prod
    ‚îÇ   ‚îî‚îÄ‚îÄ Sub-Dev
    ‚îî‚îÄ‚îÄ Security
        ‚îî‚îÄ‚îÄ Sub-SecurityOps
```

**Requirement**:
You want to enforce:

* **Allowed locations**: Only India regions (Central India, South India)
* **Mandatory tags**: Environment, CostCenter
* **Deny public IPs on NICs**

**Action**:
‚úÖ Assign **Azure Policy definitions** at the **Management Group** `Corp-IT` ‚Äî this ensures:

* All subscriptions under `Corp-IT` follow the policy.
* Any new subscription added under this MG will also comply **automatically**.

---

### ‚úÖ Summary

| Scope Level          | Policy Application Use                           |
| -------------------- | ------------------------------------------------ |
| **Management Group** | ‚úÖ Best for enterprise-wide governance            |
| Subscription         | ‚úÖ Fine for isolated control                      |
| Resource Group       | üö´ Limited ‚Äî not scalable for org-wide control   |
| Individual Resource  | ‚ùå Not recommended ‚Äî too granular, hard to manage |

---

‚úÖ **Yes, absolutely.**
When you assign a policy at the **Management Group** or **Subscription** level in Azure, that policy **automatically applies** to all **Resource Groups and Resources** **within that scope**, unless explicitly excluded.

---

### üîÑ Policy Inheritance in Azure

| Assigned At          | Applies To                                                |
| -------------------- | --------------------------------------------------------- |
| **Management Group** | ‚Üí All Subscriptions ‚Üí All Resource Groups ‚Üí All Resources |
| **Subscription**     | ‚Üí All Resource Groups ‚Üí All Resources                     |
| **Resource Group**   | ‚Üí Only resources within that RG                           |

---

### üß† Example

You assign this policy:

```json
"policyDefinitionId": "/providers/Microsoft.Authorization/policyDefinitions/allowedLocations"
```

at the **Subscription** level ‚Äî say `Sub-Production`.

Then it will:

* **Apply to all Resource Groups** like `RG-App1`, `RG-DB`, `RG-AKS`
* And all resources (VMs, Storage, AKS, etc.) under those RGs
* **Enforce during both manual and IaC deployments**

---

### üîí Important Notes

* ‚úîÔ∏è **Policy enforcement is automatic and real-time** at lower scopes.
* ‚ùå **You cannot break inheritance** unless you explicitly **exclude scopes** during assignment.
* ‚úÖ You can **override** a higher-scope policy with **a different assignment** at a **lower scope**, depending on policy effects (like `audit`, `deny`, etc.).

---

### üß™ Pro Tip

To **verify which policies are impacting a resource**, use:

```bash
az policy state list --resource <resource-id>
```

or in Azure Portal:

> Go to **Resource ‚Üí Policies** tab ‚Üí View **Effective policies**

---

Important facts about management groups:

10,000 management groups can be supported in a single directory.
A management group tree can support up to six levels of depth. This limit doesn't include the root level or the subscription level.
Each management group and subscription can support only one parent.

Yes, that's absolutely correct ‚úÖ

### üîπ Azure Management Group Tree ‚Äì Depth Limit:

* ‚úÖ **Max depth = 6 levels**
* ‚ùå This **does not include** the **root level** (the topmost management group) or the **subscription level**.

---

### üî∏ Real Example:

Suppose you‚Äôre working in a large enterprise (like LTI Mindtree or CDK Global) with this hierarchy:

```
Root Management Group (Not counted)
‚îú‚îÄ‚îÄ Level 1: Contoso Corp
‚îÇ   ‚îú‚îÄ‚îÄ Level 2: IT Division
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Level 3: App Teams
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Level 4: Dev
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Level 5: Test
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Level 6: Prod
‚îÇ   ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ EA Subscription
```

üëâ You‚Äôve now reached the **maximum allowed 6 levels** before the subscription.
Any attempt to add another level will result in an error.

---

### üß† Why this limit?

To avoid:

* Policy evaluation complexity
* Performance degradation in RBAC inheritance
* Infinite loops in governance logic


Azure CLI Basics 

Az login - Select the account you want to log in with

PS C:\Users\Srigirirajaraghuram> az account show
{
  "environmentName": "AzureCloud",
  "homeTenantId": "153fe4a0-672d-420f-a0f7-ba730e7f72e7",
  "id": "b8b28901-2bea-4bea-9868-7375c38fd149",
  "isDefault": true,
  "managedByTenants": [],
  "name": "Azure subscription 1",
  "state": "Enabled",
  "tenantDefaultDomain": "simpleram14gmail.onmicrosoft.com",
  "tenantDisplayName": "Default Directory",
  "tenantId": "153fe4a0-672d-420f-a0f7-ba730e7f72e7",
  "user": {
    "name": "simpleram14@gmail.com",
    "type": "user"
  }
}

Find commands 
az find vm - Use the az find command. For example, to search for command names containing vm.

az vm availability-set --help 

PS C:\Users\Srigirirajaraghuram> az vm availability-set --help

Group
    az vm availability-set : Group resources into availability sets.
        To provide redundancy to an application, it is recommended to group two or more virtual
        machines in an availability set. This configuration ensures that during either a planned or
        unplanned maintenance event, at least one virtual machine will be available.

Commands:
    convert    : Convert an Azure Availability Set to contain VMs with managed disks.
    create     : Create an Azure Availability Set.
    delete     : Delete an availability set.
    list       : List all availability sets in a subscription.
    list-sizes : List all available virtual machine sizes that can be used to create a new virtual
                 machine in an existing availability set.
    show       : Get information about an availability set.
    update     : Update an Azure Availability Set.

To search AI knowledge base for examples, use: az find "az vm availability-set"

az vm create --help - You can also use --help to get parameter lists and command examples for a reference command.
Argument	Description
--help	View command help
--output	Change output format: json, jsonc, tsv, table, yaml
--query	Filter output using JMESPath
--verbose	Print more execution details
--debug	Show low-level REST calls for debugging
--subscription	Specify subscription name or ID
--only-show-errors	Suppress noncritical output

Run Interactive mode with : az interactive 

Azure CLI: Automates tasks, e.g., az group create --name guardian-rg --location eastus
Why CLI? Saves time, reduces errors in TCS projects, aligns with East US region.
Logged into CLI 2.75.0 with subscription b8b28901-2bea-4bea-9868-7375c38fd149.

Azure CLI Commands:
- az group create --name guardian-rg --location eastus : Creates resource group.
- az group list --output table: Shows readable list.
- --query "[].name": Filters names, e.g., az group list --query "[].name" --output tsv.
CLI 2.75.0 in C:\Program Files (x86)\AzureCli, subscription b8b28901-2bea-4bea-9868-7375c38fd149 set at 4:15 PM.
az find failed due to DNS error, using --help and Microsoft Learn instead.

PS C:\Users\Srigirirajaraghuram> az group create --name guardian-rg --location "South India"
{
  "id": "/subscriptions/b8b28901-2bea-4bea-9868-7375c38fd149/resourceGroups/guardian-rg",
  "location": "southindia",
  "managedBy": null,
  "name": "guardian-rg",
  "properties": {
    "provisioningState": "Succeeded"
  },
  "tags": null,
  "type": "Microsoft.Resources/resourceGroups"
}
PS C:\Users\Srigirirajaraghuram> az group --help

Group
    az group : Manage resource groups and template deployments.

Subgroups:
    lock   : Manage Azure resource group locks.

Commands:
    create : Create a new resource group.
    delete : Delete a resource group.
    exists : Check if a resource group exists.
    export : Captures a resource group as a template.
    list   : List resource groups.
    show   : Gets a resource group.
    update : Update a resource group.
    wait   : Place the CLI in a waiting state until a condition of the resource group is met.

To search AI knowledge base for examples, use: az find "az group"

PS C:\Users\Srigirirajaraghuram> az group list
[
  {
    "id": "/subscriptions/b8b28901-2bea-4bea-9868-7375c38fd149/resourceGroups/guardian-rg",
    "location": "southindia",
    "managedBy": null,
    "name": "guardian-rg",
    "properties": {
      "provisioningState": "Succeeded"
    },
    "tags": null,
    "type": "Microsoft.Resources/resourceGroups"
  }
]
PS C:\Users\Srigirirajaraghuram> az group show
the following arguments are required: --name/-n/--resource-group/-g

Examples from AI knowledge base:
https://aka.ms/cli_ref
Read more about the command in reference docs
PS C:\Users\Srigirirajaraghuram> az group show  --name/-n/--resource-group/-g
the following arguments are required: --name/-n/--resource-group/-g

Examples from AI knowledge base:
https://aka.ms/cli_ref
Read more about the command in reference docs
PS C:\Users\Srigirirajaraghuram> az group show  --name/-n/--guardian-rg/-g
the following arguments are required: --name/-n/--resource-group/-g

Examples from AI knowledge base:
az group show --resource-group myresourcegroup
Gets a resource group. (autogenerated)

az account show
Get the details of a subscription. (autogenerated)

az group list --query "[?location=='westus']"
List all resource groups located in the West US region.

https://docs.microsoft.com/en-US/cli/azure/group#az_group_show
Read more about the command in reference docs
PS C:\Users\Srigirirajaraghuram>

az group show --name guardian-rg --output table - Shows the details of guardian-rg

az group show
learn.microsoft.com/en-us/cli/azure/group#az-group-show 

Azure CLI Commands:
- az group create --name guardian-rg --location southindia : Created resource group at 4:56 PM.
- az group show --name guardian-rg  : Shows details, use --name or -g, not combined.
- az group list --output table : Lists resource groups (guardian-rg in southindia).
- --query "[].name": Filters names, e.g., az group list --query "[].name" --output tsv.
CLI 2.75.0 in C:\Program Files (x86)\AzureCli, subscription b8b28901-2bea-4bea-9868-7375c38fd149.
Bypassed az find DNS error using --help and Microsoft Learn.

PS C:\Users\Srigirirajaraghuram> az group create --name guardian-rg --location "South India"
{
  "id": "/subscriptions/b8b28901-2bea-4bea-9868-7375c38fd149/resourceGroups/guardian-rg",
  "location": "southindia",
  "managedBy": null,
  "name": "guardian-rg",
  "properties": {
    "provisioningState": "Succeeded"
  },
  "tags": null,
  "type": "Microsoft.Resources/resourceGroups"
}
PS C:\Users\Srigirirajaraghuram> az group show -n guardian-rg
{
  "id": "/subscriptions/b8b28901-2bea-4bea-9868-7375c38fd149/resourceGroups/guardian-rg",
  "location": "southindia",
  "managedBy": null,
  "name": "guardian-rg",
  "properties": {
    "provisioningState": "Succeeded"
  },
  "tags": null,
  "type": "Microsoft.Resources/resourceGroups"
}
PS C:\Users\Srigirirajaraghuram> az group list --output table
Name         Location    Status
-----------  ----------  ---------
guardian-rg  southindia  Succeeded
PS C:\Users\Srigirirajaraghuram>

Review of Completed Tasks
CLI Commands:
az group create --name guardian-rg --location "South India": Created guardian-rg at 4:56 PM, confirmed in South India.
az group show -n guardian-rg: Successfully displayed details of guardian-rg.
az group list --output table: Listed resource groups, showing guardian-rg with Succeeded status.
Notes: Updated notes.txt with CLI commands, arguments (--query "[].name", --output tsv), and az find workaround.
Learning: Reviewed learn.microsoft.com/en-us/cli/azure/group#az-group-show, mastering --name/-n/-g syntax.
Real-Time Context: At TCS, accurate CLI usage (e.g., az group show -n) reduces setup errors by 20%, similar to your LTIMindtree VNet automation.
Checkpoint: guardian-rg exists, CLI commands work, and you‚Äôre ready to proceed.
Note: Running az group create --name guardian-rg again (as shown) didn‚Äôt create a duplicate because Azure CLI skips if the resource group exists, returning the existing group‚Äôs details. For guardian-test-rg, you‚Äôll create a new resource group.

az group create --name guardian-test-rg --location southindia --output table
az group show -n guardian-test-rg --output table

PS C:\Users\Srigirirajaraghuram> az group create --name guardian-test-rg --location southindia --output table
Location    Name
----------  ----------------
southindia  guardian-test-rg
PS C:\Users\Srigirirajaraghuram> az group show --name guardian-test-rg --output table
Location    Name
----------  ----------------
southindia  guardian-test-rg

CLI Success: Accurate az group show -n guardian-rg at 5:17 PM, reduces errors like LTIMindtree VNet automation!

az group list --query "[?location=='southindia'].{Name:name, Location:location}" --output table - filtering and formatting outputs, which is critical for TCS automation.
PS C:\Users\Srigirirajaraghuram> az group list --query "[?location=='southindia'].{Name:name, Location:location}" --output table
Name              Location
----------------  ----------
guardian-rg       southindia
guardian-test-rg  southindia
PS C:\Users\Srigirirajaraghuram> az

Git - Which is a software and application we can install it our local machines to mange changes to project that traks and manages we can see in futures in future.
Git hub - is and cloud based application and website , what git is doing in the local machine what ever the work we do we actaully call it as git repository the same job git hub will do in online that means in cloud ,  we can now upload and store the repository in online so every one working in that specific project can see it work track online remotely as the access levels in org in real time this works in a way .

Git Configurations 
++ System Level 
++ user level (Global)
++ Repository Level 

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 ~
$ git config --system --list
diff.astextplain.textconv=astextplain
filter.lfs.clean=git-lfs clean -- %f
filter.lfs.smudge=git-lfs smudge -- %f
filter.lfs.process=git-lfs filter-process
filter.lfs.required=true
http.sslbackend=openssl
http.sslcainfo=C:/Program Files/Git/mingw64/etc/ssl/certs/ca-bundle.crt
core.autocrlf=true
core.fscache=true
core.symlinks=false
pull.rebase=false
credential.helper=manager
credential.https://dev.azure.com.usehttppath=true
init.defaultbranch=master

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 ~
$ git config --global --list
filter.lfs.clean=git-lfs clean -- %f
filter.lfs.smudge=git-lfs smudge -- %f
filter.lfs.process=git-lfs filter-process
filter.lfs.required=true
user.name=Raghuram Srigiris
user.email=129198300+Srigiriraja@users.noreply.github.com

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 ~
$
Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 ~
$ ^C

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 ~
$ git config --global user,name "Simpleram"
error: key does not contain a section: user,name

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 ~
$ git config --global user.name "Simpleram"

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 ~
$ git config --global user.email "Simpleram"

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 ~
$ git config --global --list
filter.lfs.clean=git-lfs clean -- %f
filter.lfs.smudge=git-lfs smudge -- %f
filter.lfs.process=git-lfs filter-process
filter.lfs.required=true
user.name=Simpleram
user.email=Simpleram

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 ~
$ git config --global user.name "Srigiriraja"

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 ~
$ git config --user.email "raghuram147y@gmail.com"
error: unknown option `user.email'
usage: git config list [<file-option>] [<display-option>] [--includes]
   or: git config get [<file-option>] [<display-option>] [--includes] [--all] [-
-regexp] [--value=<value>] [--fixed-value] [--default=<default>] <name>
   or: git config set [<file-option>] [--type=<type>] [--all] [--value=<value>]
[--fixed-value] <name> <value>
   or: git config unset [<file-option>] [--all] [--value=<value>] [--fixed-value
] <name>
   or: git config rename-section [<file-option>] <old-name> <new-name>
   or: git config remove-section [<file-option>] <name>
   or: git config edit [<file-option>]
   or: git config [<file-option>] --get-colorbool <name> [<stdout-is-tty>]

Config file location
    --[no-]global         use global config file
    --[no-]system         use system config file
    --[no-]local          use repository config file
    --[no-]worktree       use per-worktree config file
    -f, --[no-]file <file>
                          use given config file
    --[no-]blob <blob-id> read config from given blob object

Action
    --get                 get value: name [<value-pattern>]
    --get-all             get all values: key [<value-pattern>]
    --get-regexp          get values for regexp: name-regex [<value-pattern>]
    --get-urlmatch        get value specific for the URL: section[.var] URL
    --replace-all         replace all matching variables: name value [<value-pat
tern>]
    --add                 add a new variable: name value
    --unset               remove a variable: name [<value-pattern>]
    --unset-all           remove all matches: name [<value-pattern>]
    --rename-section      rename section: old-name new-name
    --remove-section      remove a section: name
    -l, --list            list all
    -e, --edit            open an editor
    --get-color           find the color configured: slot [<default>]
    --get-colorbool       find the color setting: slot [<stdout-is-tty>]

Display options
    -z, --[no-]null       terminate values with NUL byte
    --[no-]name-only      show variable names only
    --[no-]show-origin    show origin of config (file, standard input, blob, com
mand line)
    --[no-]show-scope     show scope of config (worktree, local, global, system,
 command)
    --[no-]show-names     show config keys in addition to their values

Type
    -t, --[no-]type <type>
                          value is given this type
    --bool                value is "true" or "false"
    --int                 value is decimal number
    --bool-or-int         value is --bool or --int
    --bool-or-str         value is --bool or string
    --path                value is a path (file or directory name)
    --expiry-date         value is an expiry date

Other
    --[no-]default <value>
                          with --get, use default value when missing entry
    --[no-]comment <value>
                          human-readable comment string (# will be prepended as
needed)
    --[no-]fixed-value    use string equality when comparing values to value pat
tern
    --[no-]includes       respect include directives on lookup


Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 ~
$ git config --user.email "raghuram147y@gmail"
error: unknown option `user.email'
usage: git config list [<file-option>] [<display-option>] [--includes]
   or: git config get [<file-option>] [<display-option>] [--includes] [--all] [-
-regexp] [--value=<value>] [--fixed-value] [--default=<default>] <name>
   or: git config set [<file-option>] [--type=<type>] [--all] [--value=<value>]
[--fixed-value] <name> <value>
   or: git config unset [<file-option>] [--all] [--value=<value>] [--fixed-value
] <name>
   or: git config rename-section [<file-option>] <old-name> <new-name>
   or: git config remove-section [<file-option>] <name>
   or: git config edit [<file-option>]
   or: git config [<file-option>] --get-colorbool <name> [<stdout-is-tty>]

Config file location
    --[no-]global         use global config file
    --[no-]system         use system config file
    --[no-]local          use repository config file
    --[no-]worktree       use per-worktree config file
    -f, --[no-]file <file>
                          use given config file
    --[no-]blob <blob-id> read config from given blob object

Action
    --get                 get value: name [<value-pattern>]
    --get-all             get all values: key [<value-pattern>]
    --get-regexp          get values for regexp: name-regex [<value-pattern>]
    --get-urlmatch        get value specific for the URL: section[.var] URL
    --replace-all         replace all matching variables: name value [<value-pat
tern>]
    --add                 add a new variable: name value
    --unset               remove a variable: name [<value-pattern>]
    --unset-all           remove all matches: name [<value-pattern>]
    --rename-section      rename section: old-name new-name
    --remove-section      remove a section: name
    -l, --list            list all
    -e, --edit            open an editor
    --get-color           find the color configured: slot [<default>]
    --get-colorbool       find the color setting: slot [<stdout-is-tty>]

Display options
    -z, --[no-]null       terminate values with NUL byte
    --[no-]name-only      show variable names only
    --[no-]show-origin    show origin of config (file, standard input, blob, com
mand line)
    --[no-]show-scope     show scope of config (worktree, local, global, system,
 command)
    --[no-]show-names     show config keys in addition to their values

Type
    -t, --[no-]type <type>
                          value is given this type
    --bool                value is "true" or "false"
    --int                 value is decimal number
    --bool-or-int         value is --bool or --int
    --bool-or-str         value is --bool or string
    --path                value is a path (file or directory name)
    --expiry-date         value is an expiry date

Other
    --[no-]default <value>
                          with --get, use default value when missing entry
    --[no-]comment <value>
                          human-readable comment string (# will be prepended as
needed)
    --[no-]fixed-value    use string equality when comparing values to value pat
tern
    --[no-]includes       respect include directives on lookup


Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 ~
$ git config --global user.email "raghuram147y@gmail"

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 ~
$ git config --global list
error: key does not contain a section: list

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 ~
$ git config --global --list
filter.lfs.clean=git-lfs clean -- %f
filter.lfs.smudge=git-lfs smudge -- %f
filter.lfs.process=git-lfs filter-process
filter.lfs.required=true
user.name=Srigiriraja
user.email=raghuram147y@gmail

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 ~
$ git config --list
diff.astextplain.textconv=astextplain
filter.lfs.clean=git-lfs clean -- %f
filter.lfs.smudge=git-lfs smudge -- %f
filter.lfs.process=git-lfs filter-process
filter.lfs.required=true
http.sslbackend=openssl
http.sslcainfo=C:/Program Files/Git/mingw64/etc/ssl/certs/ca-bundle.crt
core.autocrlf=true
core.fscache=true
core.symlinks=false
pull.rebase=false
credential.helper=manager
credential.https://dev.azure.com.usehttppath=true
init.defaultbranch=master
filter.lfs.clean=git-lfs clean -- %f
filter.lfs.smudge=git-lfs smudge -- %f
filter.lfs.process=git-lfs filter-process
filter.lfs.required=true
user.name=Srigiriraja
user.email=raghuram147y@gmail

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 ~
$ Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 ~
$ ^C

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 ~
$ git config --global user,name "Simpleram"
error: key does not contain a section: user,name

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 ~
$ git config --global user.name "Simpleram"

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 ~
$ git config --global user.email "Simpleram"

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 ~
$ git config --global --list
filter.lfs.clean=git-lfs clean -- %f
filter.lfs.smudge=git-lfs smudge -- %f
filter.lfs.process=git-lfs filter-process
filter.lfs.required=true
user.name=Simpleram
user.email=Simpleram
Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 ~
$ ^C

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 ~
$ git config --global user,name "Simpleram"
error: key does not contain a section: user,name

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 ~
$ git config --global user.name "Simpleram"

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 ~
$ git config --global user.email "Simpleram"

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 ~
$ git config --global --list
filter.lfs.clean=git-lfs clean -- %f
filter.lfs.smudge=git-lfs smudge -- %f
filter.lfs.process=git-lfs filter-process
filter.lfs.required=true
user.name=Simpleram
user.email=Simpleram

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 ~
$ git config --global user.name "Srigiriraja"

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 ~
$ git config --user.email "raghuram147y@gmail.com"
error: unknown option `user.email'
usage: git config list [<file-option>] [<display-option>] [--includes]
Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 ~y@gmail"on lookupalues to value pat
bash: Srigirirajaraghuram@DESKTOP-7QG8BH0: command not found
bash: $: command not found

bash: Srigirirajaraghuram@DESKTOP-7QG8BH0: command not found
bash: $: command not found
bash: error:: command not found
bash: Srigirirajaraghuram@DESKTOP-7QG8BH0: command not found
bash: $: command not found
bash: Srigirirajaraghuram@DESKTOP-7QG8BH0: command not found
bash: $: command not found
bash: Srigirirajaraghuram@DESKTOP-7QG8BH0: command not found
bash: $: command not found
bash: filter.lfs.clean=git-lfs: command not found
bash: filter.lfs.smudge=git-lfs: command not found
bash: filter.lfs.process=git-lfs: command not found
bash: filter.lfs.required=true: command not found
bash: user.name=Simpleram: command not found

bash: user.email=Simpleram: command not found
bash: Srigirirajaraghuram@DESKTOP-7QG8BH0: command not found
bash: $: command not found
bash: Srigirirajaraghuram@DESKTOP-7QG8BH0: command not found
bash: $: command not found
bash: error:: command not found
bash: Srigirirajaraghuram@DESKTOP-7QG8BH0: command not found
bash: $: command not found
bash: Srigirirajaraghuram@DESKTOP-7QG8BH0: command not found
bash: $: command not found
bash: Srigirirajaraghuram@DESKTOP-7QG8BH0: command not found
bash: $: command not found
bash: filter.lfs.clean=git-lfs: command not found
bash: filter.lfs.smudge=git-lfs: command not found
bash: filter.lfs.process=git-lfs: command not found
bash: filter.lfs.required=true: command not found
bash: user.name=Simpleram: command not found
bash: user.email=Simpleram: command not found
bash: Srigirirajaraghuram@DESKTOP-7QG8BH0: command not found
bash: $: command not found
bash: Srigirirajaraghuram@DESKTOP-7QG8BH0: command not found
bash: $: command not found
> git config --list
> ^C

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 ~
$ git global --list
git: 'global' is not a git command. See 'git --help'.

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 ~
$ git config --list
diff.astextplain.textconv=astextplain
filter.lfs.clean=git-lfs clean -- %f
filter.lfs.smudge=git-lfs smudge -- %f
filter.lfs.process=git-lfs filter-process
filter.lfs.required=true
http.sslbackend=openssl
http.sslcainfo=C:/Program Files/Git/mingw64/etc/ssl/certs/ca-bundle.crt
core.autocrlf=true
core.fscache=true
core.symlinks=false
pull.rebase=false
credential.helper=manager
credential.https://dev.azure.com.usehttppath=true
init.defaultbranch=master
filter.lfs.clean=git-lfs clean -- %f
filter.lfs.smudge=git-lfs smudge -- %f
filter.lfs.process=git-lfs filter-process
filter.lfs.required=true
user.name=Srigiriraja
user.email=raghuram147y@gmail

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 ~
$ =true
user.name=Srigiriraja
user.email=raghuram147y@gmail

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 ~

Display all 53 possibilities? (y or n)

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 ~
$ =true
user.name=Srigiriraja
user.email=raghuram147y@gmail

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 ~
bash: =true: command not found
bash: user.name=Srigiriraja: command not found
bash: user.email=raghuram147y@gmail: command not found
bash: Srigirirajaraghuram@DESKTOP-7QG8BH0: command not found

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 ~
$

Git WorkFlow / stages 

By using git managing our projects in different versions , code tacking , there is procedure git will not track directly , we need some basic basic commads.

Git workflow 
Local 
Working directly ->(cmd : git init git add) staging / Index -> (cmd:Git commit) Local repository .

Then from local machine same folder which we have worked on those updates / sends / move to  remote cloud for back up / tracking / working on changes / RBAC for working on progress in real time 

Cloud Repository (cmd : Git push) - Github , Gitlab , Bitbucket etc 

In real time we should not be there form beginning of the project or completely configured from scratch we should come in middle then , some one who worked on before will stores the project in cloud repo then we have to pull it to our local machine the need to stsrt work on 
For that purpose 
Git Clone 

Git commands 
Git status
Gt pull 
Git init 
git add 
git commit -m "messge "
git log 
git push 

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 ~
$ ^C

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 ~
$ /c/T19/JavaProject
bash: /c/T19/JavaProject: Is a directory

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 ~
$ cd  c
bash: cd: c: No such file or directory

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 ~
$ cd

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 ~
$ ld
bash: ld: command not found

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 ~
$ ls
'3D Objects'/         Downloads/               NTUSER.DAT{53b39e88-18c4-11ea-a811-000d3aa4692b}.TM.blf                                        Searches/             docker_volumes/
 ADO/                 Favorites/               NTUSER.DAT{53b39e88-18c4-11ea-a811-000d3aa4692b}.TMContainer00000000000000000001.regtrans-ms   SendTo@               files/
 AppData/             IntelGraphicsProfiles/   NTUSER.DAT{53b39e88-18c4-11ea-a811-000d3aa4692b}.TMContainer00000000000000000002.regtrans-ms  'Start Menu'@          ntuser.dat.LOG1
'Application Data'@   Links/                   NetHood@                                                                                       Templates@            ntuser.dat.LOG2
 Contacts/           'Local Settings'@         OneDrive/                                                                                      Videos/               ntuser.ini
 Cookies@             Music/                   PrintHood@                                                                                    'VirtualBox VMs'/      source/
 Dockerfiles/        'My Documents'@           Recent@                                                                                        docker/
 Documents/           NTUSER.DAT              'Saved Games'/                                                                                  docker-compose-app/

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 ~
$ c/T19/JavaProject (master)
bash: syntax error near unexpected token `master'

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 ~
$ cd c/T19/JavaProject (master)
bash: syntax error near unexpected token `('

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 ~
$ cd c/T19/JavaProject
bash: cd: c/T19/JavaProject: No such file or directory

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 ~
$ cd ..

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/Users
$ ls
'All Users'@   Default/  'Default User'@   Public/   Srigirirajaraghuram/   desktop.ini

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/Users
$ cd ..

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c
$ ls
'$Recycle.Bin'/                      Class_Code/                DumpStack.log       OneDriveTemp/           ProgramData/                  Terraform/     Windows.old.000/   kubectl/
'$WINRE_BACKUP_PARTITION.MARKER'     Dell/                      DumpStack.log.tmp   PerfLogs/               Recovery/                     Users/         bootTel.dat        pagefile.sys
'$WinREAgent'/                      'DevOps Resumes'/           Intel/             'Program Files'/        'System Volume Information'/   Windows/       hiberfil.sys       swapfile.sys
 AzureAdminManualTaskstoTerraform/  'Documents and Settings'@   MSOCache/          'Program Files (x86)'/   T19/                          Windows.old/   inetpub/          '~ai-guardiandocs'/

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c
$ cd T19

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19 (main)
$ ls
Ansible/  Docker/  DotnetProject/  JavaProject/  Kubernetes/  README.md  Terraform/  TravelApplicationproject/

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19 (main)
$ cd ^C

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19 (main)
$ cd JavaProject

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/JavaProject (master)
$ ls
mvn_sample_app/

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/JavaProject (master)


Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/JavaProject (master)
$ ls
mvn_sample_app/

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/JavaProject (master)
$ cd mvn_sample_app/

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/JavaProject/mvn_sample_app (main|REBASE 5/9)
$ ls
README.md  mvnw*  mvnw.cmd  package.json  pom.xml  src/  styles/  tailwind.config.js  yarn.lock

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/JavaProject/mvn_sample_app (main|REBASE 5/9)
$ ^C

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/JavaProject/mvn_sample_app (main|REBASE 5/9)
$ cd ..

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/JavaProject (master)
$ git init
Reinitialized existing Git repository in C:/T19/JavaProject/.git/

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/JavaProject (master)

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/JavaProject (master)
$ git status
On branch master

No commits yet

Changes to be committed:
  (use "git rm --cached <file>..." to unstage)
        new file:   mvn_sample_app

Changes not staged for commit:
  (use "git add <file>..." to update what will be committed)
  (use "git restore <file>..." to discard changes in working directory)
  (commit or discard the untracked or modified content in submodules)
        modified:   mvn_sample_app (new commits, modified content)


Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/JavaProject (master)
$ git commit -m "This java project is commiting intially only using the basic commands repository needs to be created ai-guardian (Public) and pushed to remote repo"
[master (root-commit) 9263d89] This java project is commiting intially only using the basic commands repository needs to be created ai-guardian (Public) and pushed to remote repo
 1 file changed, 1 insertion(+)
 create mode 160000 mvn_sample_app

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/JavaProject (master)


Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/JavaProject/mvn_sample_app (main|REBASE 6/9)
$ git status
interactive rebase in progress; onto 6014bd9
Last commands done (6 commands done):
   pick ebd219d feat: add the readme
   pick a2977b1 feat: build the docker image
  (see more in file .git/rebase-merge/done)
Next commands to do (3 remaining commands):
   pick d894616 feat: delete  dockerfile
   pick cb7aa97 chore: standalone css generation
  (use "git rebase --edit-todo" to view and edit)
You are currently rebasing branch 'main' on '6014bd9'.
  (fix conflicts and then run "git rebase --continue")
  (use "git rebase --skip" to skip this patch)
  (use "git rebase --abort" to check out the original branch)

Changes to be committed:
  (use "git restore --staged <file>..." to unstage)
        new file:   Dockerfile

Unmerged paths:
  (use "git restore --staged <file>..." to unstage)
  (use "git add <file>..." to mark resolution)
        both modified:   README.md


Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/JavaProject/mvn_sample_app (main|REBASE 6/9)
$ ^C

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/JavaProject/mvn_sample_app (main|REBASE 6/9)
$ ls
Dockerfile  README.md  mvnw*  mvnw.cmd  package.json  pom.xml  src/  styles/  tailwind.config.js  yarn.lock

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/JavaProject/mvn_sample_app (main|REBASE 6/9)
$ cd README.md
bash: cd: README.md: Not a directory

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/JavaProject/mvn_sample_app (main|REBASE 6/9)
$ ^C

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/JavaProject/mvn_sample_app (main|REBASE 6/9)
$ cd README.md
bash: cd: README.md: Not a directory

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/JavaProject/mvn_sample_app (main|REBASE 6/9)
$ ^C

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/JavaProject/mvn_sample_app (main|REBASE 6/9)
$ code README.md

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/JavaProject/mvn_sample_app (main|REBASE 6/9)
$ cd ..

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/JavaProject (master)
$ git status
On branch master
Changes to be committed:
  (use "git restore --staged <file>..." to unstage)
        modified:   mvn_sample_app

Changes not staged for commit:
  (use "git add <file>..." to update what will be committed)
  (use "git restore <file>..." to discard changes in working directory)
  (commit or discard the untracked or modified content in submodules)
        modified:   mvn_sample_app (new commits, modified content)


Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/JavaProject (master)
$ git commit -m "This java project is commiting intially only using the basic commands repository needs to be created ai-guardian (Public) and pushed to remote repo"
[master 3b76578] This java project is commiting intially only using the basic commands repository needs to be created ai-guardian (Public) and pushed to remote repo
 1 file changed, 1 insertion(+), 1 deletion(-)

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/JavaProject (master)
$ git status
On branch master
Changes not staged for commit:
  (use "git add <file>..." to update what will be committed)
  (use "git restore <file>..." to discard changes in working directory)
  (commit or discard the untracked or modified content in submodules)
        modified:   mvn_sample_app (new commits, modified content)

no changes added to commit (use "git add" and/or "git commit -a")

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/JavaProject (master)
$ git add mvn_sample_app

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/JavaProject (master)
$ git commit -m "This java project is commiting intially only using the basic commands repository needs to be created ai-guardian (Public) and pushed to remote repo"
[master 26157ab] This java project is commiting intially only using the basic commands repository needs to be created ai-guardian (Public) and pushed to remote repo
 1 file changed, 1 insertion(+), 1 deletion(-)

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/JavaProject (master)
$ git status
On branch master
Changes not staged for commit:
  (use "git add <file>..." to update what will be committed)
  (use "git restore <file>..." to discard changes in working directory)
  (commit or discard the untracked or modified content in submodules)
        modified:   mvn_sample_app (modified content)

no changes added to commit (use "git add" and/or "git commit -a")

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/JavaProject (master)
$ git commi t-a
git: 'commi' is not a git command. See 'git --help'.

The most similar commands are
        commit
        column
        config

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/JavaProject (master)
$ git commit -a
On branch master
Changes not staged for commit:
  (use "git add <file>..." to update what will be committed)
  (use "git restore <file>..." to discard changes in working directory)
  (commit or discard the untracked or modified content in submodules)
        modified:   mvn_sample_app (modified content)

no changes added to commit (use "git add" and/or "git commit -a")

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/JavaProject (master)
$ git log
commit 26157abdedd953e59f0410da599a75e325d64b98 (HEAD -> master)
Author: Srigiriraja <raghuram147y@gmail>
Date:   Mon Jul 14 15:03:04 2025 +0530

    This java project is commiting intially only using the basic commands repository needs to be created ai-guardian (Public) and pushed to remote repo

commit 3b7657836cdf20a6d81462b536cbb98b513f791f
Author: Srigiriraja <raghuram147y@gmail>
Date:   Mon Jul 14 15:02:21 2025 +0530

    This java project is commiting intially only using the basic commands repository needs to be created ai-guardian (Public) and pushed to remote repo

commit 9263d89680c029da250305095dd960bd47795c7d
Author: Srigiriraja <raghuram147y@gmail>
Date:   Mon Jul 14 13:17:31 2025 +0530

    This java project is commiting intially only using the basic commands repository needs to be created ai-guardian (Public) and pushed to remote repo

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/JavaProject (master)



[detached HEAD 4fc5f25] feat: build the docker image
 Author: Eric Cabrel TIOGO <tericcabrel@yahoo.com>
 2 files changed, 46 insertions(+)
 create mode 100644 Dockerfile
Auto-merging README.md
CONFLICT (content): Merge conflict in README.md
error: could not apply d894616... feat: delete  dockerfile
hint: Resolve all conflicts manually, mark them as resolved with
hint: "git add/rm <conflicted_files>", then run "git rebase --continue".
hint: You can instead skip this commit: run "git rebase --skip".
hint: To abort and get back to the state before "git rebase", run "git rebase --abort".
hint: Disable this message with "git config advice.mergeConflict false"
Could not apply d894616... feat: delete  dockerfile

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/JavaProject/mvn_sample_app (main|REBASE 7/9)
$ git status
interactive rebase in progress; onto 6014bd9
Last commands done (7 commands done):
   pick a2977b1 feat: build the docker image
   pick d894616 feat: delete  dockerfile
  (see more in file .git/rebase-merge/done)
Next commands to do (2 remaining commands):
   pick cb7aa97 chore: standalone css generation
   pick a526a29 Add files via upload
  (use "git rebase --edit-todo" to view and edit)
You are currently rebasing branch 'main' on '6014bd9'.
  (fix conflicts and then run "git rebase --continue")
  (use "git rebase --skip" to skip this patch)
  (use "git rebase --abort" to check out the original branch)

Changes to be committed:
  (use "git restore --staged <file>..." to unstage)
        deleted:    Dockerfile

Unmerged paths:
  (use "git restore --staged <file>..." to unstage)
  (use "git add <file>..." to mark resolution)
        both modified:   README.md


Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/JavaProject/mvn_sample_app (main|REBASE 7/9)

[detached HEAD 4fc5f25] feat: build the docker image
 Author: Eric Cabrel TIOGO <tericcabrel@yahoo.com>
 2 files changed, 46 insertions(+)
 create mode 100644 Dockerfile
Auto-merging README.md
CONFLICT (content): Merge conflict in README.md
error: could not apply d894616... feat: delete  dockerfile
hint: Resolve all conflicts manually, mark them as resolved with
hint: "git add/rm <conflicted_files>", then run "git rebase --continue".
hint: You can instead skip this commit: run "git rebase --skip".
hint: To abort and get back to the state before "git rebase", run "git rebase --abort".
hint: Disable this message with "git config advice.mergeConflict false"
Could not apply d894616... feat: delete  dockerfile

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/JavaProject/mvn_sample_app (main|REBASE 7/9)
$ git status
interactive rebase in progress; onto 6014bd9
Last commands done (7 commands done):
   pick a2977b1 feat: build the docker image
   pick d894616 feat: delete  dockerfile
  (see more in file .git/rebase-merge/done)
Next commands to do (2 remaining commands):
   pick cb7aa97 chore: standalone css generation
   pick a526a29 Add files via upload
  (use "git rebase --edit-todo" to view and edit)
You are currently rebasing branch 'main' on '6014bd9'.
  (fix conflicts and then run "git rebase --continue")
^Cck cb7aa97 chore: standalone css generationoriginal branch)

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/JavaProject/mvn_sample_app (main)
$ git status
On branch main
Your branch and 'origin/main' have diverged,
and have 10 and 10 different commits each, respectively.
  (use "git pull" if you want to integrate the remote branch with yours)

nothing to commit, working tree clean

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/JavaProject/mvn_sample_app (main)
Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/JavaProject/mvn_sample_app (main)
$ git log
commit 61ec15cbb2676c73aed04b7980cec13295ad0974 (HEAD -> main)
Author: nextopsvideos <94779550+nextopsvideos@users.noreply.github.com>
Date:   Thu Jun 20 18:36:01 2024 +0700

    Add files via upload

commit 5cce1b6928a3b397e40886bc547406a559459d63
Author: Eric Cabrel TIOGO <tericcabrel@yahoo.com>
Date:   Sat Jul 15 13:32:32 2023 +0200

    chore: standalone css generation

commit 452794d28753b42fff39b53338a0c1bdedc027f0
Author: Eric Cabrel TIOGO <tericcabrel@yahoo.com>
Date:   Sun May 15 15:35:35 2022 +0200

    feat: delete  dockerfile

commit 4fc5f25ce0eca57829b8906fa5d4e44fc6196512
Author: Eric Cabrel TIOGO <tericcabrel@yahoo.com>
Date:   Sun May 15 14:34:06 2022 +0200

    feat: build the docker image

commit 10c51ebf71563b3441ea2a7147c3c2a2daf4384e
Author: Eric Cabrel TIOGO <tericcabrel@yahoo.com>
Date:   Sun Apr 24 15:35:05 2022 +0200

    feat: add the readme

commit 445d25e7181245de33b5829362c85650bdc54645
Author: Eric Cabrel TIOGO <tericcabrel@yahoo.com>
Date:   Sun Apr 24 15:17:52 2022 +0200

    feat: minor improvements of the application

commit b06948c6ede604362c3e5dd7a6c50b2981272ddc
Author: Eric Cabrel TIOGO <tericcabrel@yahoo.com>
Date:   Sun Apr 24 13:19:43 2022 +0200

    feat: write dto and the ui

commit 5709ad9eefb02c635df374e0302b4147ac8943e6
Author: Eric Cabrel TIOGO <tericcabrel@yahoo.com>
Date:   Sat Apr 23 22:00:30 2022 +0200

    feat: build the form page

commit 7c6d44a00db854a8278f4df9072bb690f31c4c44
Author: Eric Cabrel TIOGO <tericcabrel@yahoo.com>
Date:   Sat Apr 23 11:43:30 2022 +0200

    feat: initial commit

commit 6014bd9b77bc9637c26620bb9ebbc24b0f1d7693
Author: Raghuram Srigiri <raghuram147y@gmail.com>
Date:   Tue Mar 11 18:05:39 2025 +0000

    Initial commit
(END)
Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/JavaProject/mvn_sample_app (main)
$ git status
On branch main
Your branch and 'origin/main' have diverged,
and have 10 and 10 different commits each, respectively.
  (use "git pull" if you want to integrate the remote branch with yours)

nothing to commit, working tree clean

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/JavaProject/mvn_sample_app (main)
$ git remote -v
origin  https://gitlab.com/dev6982052/bmi_mavenapp.git (fetch)
origin  https://gitlab.com/dev6982052/bmi_mavenapp.git (push)

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/JavaProject/mvn_sample_app (main)
$ git pull --rebase
remote: The project you were looking for could not be found or you don't have permission to view it.
fatal: repository 'https://gitlab.com/dev6982052/bmi_mavenapp.git/' not found

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/JavaProject/mvn_sample_app (main)
$ git remote add origin https://github.com/Srigiriraja/ai-guardian.git
error: remote origin already exists.

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/JavaProject/mvn_sample_app (main)

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/JavaProject/mvn_sample_app (main)
$ git remote remove origin

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/JavaProject/mvn_sample_app (main)
$ git remote -v

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/JavaProject/mvn_sample_app (main)
$ git remote add origin https://github.com/Srigiriraja/ai-guardian.git

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/JavaProject/mvn_sample_app (main)
$ git remote -v
origin  https://github.com/Srigiriraja/ai-guardian.git (fetch)
origin  https://github.com/Srigiriraja/ai-guardian.git (push)

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/JavaProject/mvn_sample_app (main)
$ git push --force origin main
info: please complete authentication in your browser...


Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/JavaProject/mvn_sample_app (main)
$ git push --force origin main
Enumerating objects: 123, done.
Counting objects: 100% (123/123), done.
Delta compression using up to 4 threads
Compressing objects: 100% (71/71), done.
Writing objects: 100% (123/123), 97.12 KiB | 2.49 MiB/s, done.
Total 123 (delta 24), reused 96 (delta 11), pack-reused 0 (from 0)
remote: Resolving deltas: 100% (24/24), done.
To https://github.com/Srigiriraja/ai-guardian.git
 + 31d8892...61ec15c main -> main (forced update)

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/JavaProject/mvn_sample_app (main)
$ git status
On branch main
nothing to commit, working tree clean

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/JavaProject/mvn_sample_app (main)
$

Issues and Fixes
Rebase Conflict (main|REBASE 7/9):
Issue: Conflict in README.md during rebase (d894616: feat: delete dockerfile conflicted with a2977b1: feat: build the docker image).
Resolution: You resolved it, and the rebase completed, returning to main branch with a clean state.
Note: The Dockerfile was deleted in the rebase, and README.md conflicts were fixed (likely manually or skipped).
GitLab Remote Failure:
Issue: git pull --rebase failed for https://gitlab.com/dev6982052/bmi_mavenapp.git (‚Äúrepository not found‚Äù).
Resolution: You removed the invalid remote (git remote remove origin) and added https://github.com/Srigiriraja/ai-guardian.
Force Push:
Action: Used git push --force origin main to overwrite the remote, resolving divergence (Your branch and 'origin/main' have diverged, and have 10 and 10 different commits each).
Note: Force push was appropriate here but use cautiously in team settings to avoid overwriting others‚Äô work.
Prompt Errors:
Issue: Commands like =true or user.name=Srigiriraja were run as input.
Fix: These were outputs from git config --list. Avoid copying outputs as commands.


++ create and configure access token PAT - done and created 
++ Git Clone - cloing a public AKS basline project to VS code and pushing to out repository
URL : https://github.com/Srigiriraja/ai-guardian
cloned URL to my local machine : https://github.com/mspnp/aks-baseline
++ We can provide access for others to works on git repo 
- Access Tokens 
- Collaberators 
++ Created - barnches and pushed it in remote repo 

C:\T19\aks-baseline>git clone https://github.com/mspnp/aks-baseline.git
Cloning into 'aks-baseline'...
remote: Enumerating objects: 2256, done.
remote: Counting objects: 100% (262/262), done.
remote: Compressing objects: 100% (148/148), done.
remote: Total 2256 (delta 197), reused 117 (delta 114), pack-reused 1994 (from 3)
Receiving objects: 100% (2256/2256), 4.28 MiB | 413.00 KiB/s, done.
Resolving deltas: 100% (1469/1469), done.

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/JavaProject/mvn_sample_app (main)
$ git branch
* main

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/JavaProject/mvn_sample_app (main)
$ git checkout main
Already on 'main'

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/JavaProject/mvn_sample_app (main)
$ git checkout -b dev
Switched to a new branch 'dev'

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/JavaProject/mvn_sample_app (dev)
$ git branch
* dev
  main

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/JavaProject/mvn_sample_app (dev)
$ git checkout dev
Already on 'dev'

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/JavaProject/mvn_sample_app (dev)
$ git checkout -b feature/aks-module
Switched to a new branch 'feature/aks-module'

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/JavaProject/mvn_sample_app (feature/aks-module)
$ git branch
  dev
* feature/aks-module
  main

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/JavaProject/mvn_sample_app (feature/aks-module)
$ touch aks.tf               # Create a sample Terraform file

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/JavaProject/mvn_sample_app (feature/aks-module)
$ git add aks.tf

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/JavaProject/mvn_sample_app (feature/aks-module)
$ git commit -m "feat: added base aks.tf for AKS setup"
[feature/aks-module c31350e] feat: added base aks.tf for AKS setup
 1 file changed, 0 insertions(+), 0 deletions(-)
 create mode 100644 aks.tf

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/JavaProject/mvn_sample_app (feature/aks-module)
$ git push origin -u feature/aks-module

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/JavaProject/mvn_sample_app (feature/aks-module)
$ git push -u origin feature/aks-module
Enumerating objects: 4, done.
Counting objects: 100% (4/4), done.
Delta compression using up to 4 threads
Compressing objects: 100% (2/2), done.
Writing objects: 100% (3/3), 291 bytes | 97.00 KiB/s, done.
Total 3 (delta 1), reused 0 (delta 0), pack-reused 0 (from 0)
remote: Resolving deltas: 100% (1/1), completed with 1 local object.
remote:
remote: Create a pull request for 'feature/aks-module' on GitHub by visiting:
remote:      https://github.com/Srigiriraja/ai-guardian/pull/new/feature/aks-module
remote:
To https://github.com/Srigiriraja/ai-guardian.git
 * [new branch]      feature/aks-module -> feature/aks-module
branch 'feature/aks-module' set up to track 'origin/feature/aks-module'.

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/JavaProject/mvn_sample_app (feature/aks-module)
$ git status
On branch feature/aks-module
Your branch is up to date with 'origin/feature/aks-module'.

nothing to commit, working tree clean

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/JavaProject/mvn_sample_app (feature/aks-module)
Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/JavaProject/mvn_sample_app (feature/aks-module)
$ ^C

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/JavaProject/mvn_sample_app (feature/aks-module)
$ git checkout dev
Switched to branch 'dev'

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/JavaProject/mvn_sample_app (dev)
$ git checkout feauture/aks-module
error: pathspec 'feauture/aks-module' did not match any file(s) known to git

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/JavaProject/mvn_sample_app (dev)
$ git checkout feature/aks-module
Switched to branch 'feature/aks-module'
Your branch is up to date with 'origin/feature/aks-module'.

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/JavaProject/mvn_sample_app (feature/aks-module)
$ gitcheck out dev
bash: gitcheck: command not found

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/JavaProject/mvn_sample_app (feature/aks-module)
$ git checkout dev
Switched to branch 'dev'

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/JavaProject/mvn_sample_app (dev)
$ git checkout -b feature/ci
Switched to a new branch 'feature/ci'

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/JavaProject/mvn_sample_app (feature/ci)
$ git branch
  dev
  feature/aks-module
* feature/ci
  main

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/JavaProject/mvn_sample_app (feature/ci)
$ mkdir -p .github/workflows

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/JavaProject/mvn_sample_app (feature/ci)
$ touch .github/workflows/deploy.yml

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/JavaProject/mvn_sample_app (feature/ci)
$ git add .

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/JavaProject/mvn_sample_app (feature/ci)
$ git commit -m "feat: add GitHub Actions workflow for CI/CD"
[feature/ci f013314] feat: add GitHub Actions workflow for CI/CD
 1 file changed, 0 insertions(+), 0 deletions(-)
 create mode 100644 .github/workflows/deploy.yml

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/JavaProject/mvn_sample_app (feature/ci)
$ git push -u origin ^C

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/JavaProject/mvn_sample_app (feature/ci)
$ git push -u origin feature/ci
Enumerating objects: 6, done.
Counting objects: 100% (6/6), done.
Delta compression using up to 4 threads
Compressing objects: 100% (2/2), done.
Writing objects: 100% (5/5), 389 bytes | 129.00 KiB/s, done.
Total 5 (delta 1), reused 0 (delta 0), pack-reused 0 (from 0)
remote: Resolving deltas: 100% (1/1), completed with 1 local object.
remote:
remote: Create a pull request for 'feature/ci' on GitHub by visiting:
remote:      https://github.com/Srigiriraja/ai-guardian/pull/new/feature/ci
remote:
To https://github.com/Srigiriraja/ai-guardian.git
 * [new branch]      feature/ci -> feature/ci
branch 'feature/ci' set up to track 'origin/feature/ci'.

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/JavaProject/mvn_sample_app (feature/ci)
$ git status
On branch feature/ci
Your branch is up to date with 'origin/feature/ci'.

nothing to commit, working tree clean

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/JavaProject/mvn_sample_app (feature/ci)
$ git log
commit f013314fc026f6958cfb1bb9a7dd90c0feb2bfee (HEAD -> feature/ci, origin/feature/ci)
Author: Srigiriraja <raghuram147y@gmail>
Date:   Mon Jul 14 20:24:04 2025 +0530

    feat: add GitHub Actions workflow for CI/CD

commit 61ec15cbb2676c73aed04b7980cec13295ad0974 (origin/main, main, dev)
Author: nextopsvideos <94779550+nextopsvideos@users.noreply.github.com>
Date:   Thu Jun 20 18:36:01 2024 +0700

    Add files via upload

commit 5cce1b6928a3b397e40886bc547406a559459d63
Author: Eric Cabrel TIOGO <tericcabrel@yahoo.com>
Date:   Sat Jul 15 13:32:32 2023 +0200

    chore: standalone css generation

commit 452794d28753b42fff39b53338a0c1bdedc027f0
Author: Eric Cabrel TIOGO <tericcabrel@yahoo.com>
Date:   Sun May 15 15:35:35 2022 +0200

    feat: delete  dockerfile

commit 4fc5f25ce0eca57829b8906fa5d4e44fc6196512
Author: Eric Cabrel TIOGO <tericcabrel@yahoo.com>
Date:   Sun May 15 14:34:06 2022 +0200

    feat: build the docker image

commit 10c51ebf71563b3441ea2a7147c3c2a2daf4384e
Author: Eric Cabrel TIOGO <tericcabrel@yahoo.com>
Date:   Sun Apr 24 15:35:05 2022 +0200

    feat: add the readme

commit 445d25e7181245de33b5829362c85650bdc54645
Author: Eric Cabrel TIOGO <tericcabrel@yahoo.com>
Date:   Sun Apr 24 15:17:52 2022 +0200

    feat: minor improvements of the application

commit b06948c6ede604362c3e5dd7a6c50b2981272ddc
Author: Eric Cabrel TIOGO <tericcabrel@yahoo.com>
Date:   Sun Apr 24 13:19:43 2022 +0200

    feat: write dto and the ui

commit 5709ad9eefb02c635df374e0302b4147ac8943e6
Author: Eric Cabrel TIOGO <tericcabrel@yahoo.com>
Date:   Sat Apr 23 22:00:30 2022 +0200


Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/JavaProject/mvn_sample_app (feature/ci)
$

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/JavaProject/mvn_sample_app (feature/ci)
$
Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/JavaProject/mvn_sample_app (feature/dev)
$ git branch
  dev
  feature/aks-module
  feature/ci
* feature/dev
  main

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/JavaProject/mvn_sample_app (feature/dev)
Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/JavaProject/mvn_sample_app (feature/ci)
$ git checkout -b future - dev
fatal: Cannot update paths and switch to branch 'future' at the same time.

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/JavaProject/mvn_sample_app (feature/ci)
$ git checkout dev
Switched to branch 'dev'

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/JavaProject/mvn_sample_app (dev)
$ git checkout -b feature/dev
Switched to a new branch 'feature/dev'

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/JavaProject/mvn_sample_app (feature/dev)
$ git branch
  dev
  feature/aks-module
  feature/ci
* feature/dev
  main

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/JavaProject/mvn_sample_app (feature/dev)
$ ^C

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/JavaProject/mvn_sample_app (feature/dev)
$ ^C

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/JavaProject/mvn_sample_app (feature/dev)
$ git add .

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/JavaProject/mvn_sample_app (feature/dev)
$ git commit -m "cloned aks-base line project and developing for K8s and terraform automation to ^C

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/JavaProject/mvn_sample_app (feature/dev)
$ git commit -m "cloned aks-base line project and developing for k8s and terraform automation to newly created feature/dev
>
>
> ^C

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/JavaProject/mvn_sample_app (feature/dev)
$ git commit -m "cloned aks-base line project and developing for k8s and terraform automation to newly created feature/dev

> git push -u origin ^C

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/JavaProject/mvn_sample_app (feature/dev)
$ git push -u origin feature/dev
Total 0 (delta 0), reused 0 (delta 0), pack-reused 0 (from 0)
remote:
remote: Create a pull request for 'feature/dev' on GitHub by visiting:
remote:      https://github.com/Srigiriraja/ai-guardian/pull/new/feature/dev
remote:
To https://github.com/Srigiriraja/ai-guardian.git
 * [new branch]      feature/dev -> feature/dev
branch 'feature/dev' set up to track 'origin/feature/dev'.

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/JavaProject/mvn_sample_app (feature/dev)
$ git remote -v
origin  https://github.com/Srigiriraja/ai-guardian.git (fetch)
origin  https://github.com/Srigiriraja/ai-guardian.git (push)


++++++++++++++++++++++

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/JavaProject/mvn_sample_app (feature/dev)
$
Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19 (main)
$ ls
Ansible/  DotnetProject/  Kubernetes/  Terraform/                 aks-baseline/
Docker/   JavaProject/    README.md    TravelApplicationproject/

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19 (main)
$ cd JavaProject

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/JavaProject (master)
$ ls
mvn_sample_app/

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/JavaProject (master)
$ cd mvn_sample_app

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/JavaProject/mvn_sample_app (feature/dev)
$ ls
Dockerfile  Dockerfile1  Dockerfile2  README.md  mvnw*  mvnw.cmd  package.json  pom.xml  src/  styles/  tailwind.config.js  yarn.lock

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/JavaProject/mvn_sample_app (feature/dev)
$ git branch
  dev
  feature/aks-module
  feature/ci
* feature/dev
  main

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/JavaProject/mvn_sample_app (feature/dev)
$ git checkout main
Switched to branch 'main'

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/JavaProject/mvn_sample_app (main)
$ git branch
  dev
  feature/aks-module
  feature/ci
  feature/dev
* main

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/JavaProject/mvn_sample_app (main)
$

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/aks-baseline (main)
$ git clone https://github.com/mspnp/aks-baseline.git
Cloning into 'aks-baseline'...
remote: Enumerating objects: 2256, done.
remote: Counting objects: 100% (261/261), done.
remote: Compressing objects: 100% (148/148), done.
remote: Total 2256 (delta 194), reused 116 (delta 113), pack-reused 1995 (from 3
)
Receiving objects: 100% (2256/2256), 4.28 MiB | 114.00 KiB/s, done.
Resolving deltas: 100% (1466/1466), done.

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/aks-baseline (main)
$ git status
On branch main
Your branch is up to date with 'origin/main'.

Untracked files:
  (use "git add <file>..." to include in what will be committed)
        ../DotnetProject/
        ../JavaProject/
        ../TravelApplicationproject/
        ./

nothing added to commit but untracked files present (use "git add" to track)

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/aks-baseline (main)
$ ^C

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/aks-baseline (main)
$ ^C

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/aks-baseline (main)
$ git add aks-baseline
warning: adding embedded git repository: aks-baseline/aks-baseline
hint: You've added another git repository inside your current repository.
hint: Clones of the outer repository will not contain the contents of
hint: the embedded repository and will not know how to obtain it.
hint: If you meant to add a submodule, use:
hint:
hint:   git submodule add <url> aks-baseline/aks-baseline
hint:
hint: If you added this path by mistake, you can remove it from the
hint: index with:
hint:
hint:   git rm --cached aks-baseline/aks-baseline
hint:
hint: See "git help submodule" for more information.
hint: Disable this message with "git config advice.addEmbeddedRepo false"

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/aks-baseline (main)
$ ^C

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/aks-baseline (main)
$ git branch
* main

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/aks-baseline (main)
$ ^C

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/aks-baseline (main)
$ git remote -v
origin  https://github.com/nextopsvideos/T19.git (fetch)
origin  https://github.com/nextopsvideos/T19.git (push)

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/aks-baseline (main)
$ git remote remove origin

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/aks-baseline (main)
$ git remote -v

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/aks-baseline (main)
$ git remote add https://github.com/Srigiriraja/ai-guardian.git
usage: git remote add [<options>] <name> <url>

    -f, --[no-]fetch      fetch the remote branches
    --[no-]tags           import all tags and associated objects when fetching
                          or do not fetch any tag at all (--no-tags)
    -t, --[no-]track <branch>
                          branch(es) to track
    -m, --[no-]master <branch>
                          master branch
    --[no-]mirror[=(push|fetch)]
                          set up remote as a mirror to push to or fetch from


Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/aks-baseline (main)
$ git remote add origin https://github.com/Srigiriraja/ai-guardian.git

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/aks-baseline (main)
$ git remote -v
origin  https://github.com/Srigiriraja/ai-guardian.git (fetch)
origin  https://github.com/Srigiriraja/ai-guardian.git (push)

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/aks-baseline (main)
$ git remote -v
origin  https://github.com/Srigiriraja/ai-guardian.git (fetch)
origin  https://github.com/Srigiriraja/ai-guardian.git (push)

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/aks-baseline (main)
$ git branch
* main

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/aks-baseline (main)
$ git status
On branch main
Changes to be committed:
  (use "git restore --staged <file>..." to unstage)
        new file:   aks-baseline

Untracked files:
  (use "git add <file>..." to include in what will be committed)
        ../DotnetProject/
        ../JavaProject/
        ../TravelApplicationproject/


Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/aks-baseline (main)
$ git add .

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/aks-baseline (main)
$ git staus
git: 'staus' is not a git command. See 'git --help'.

The most similar command is
        status

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/aks-baseline (main)
$ git status
On branch main
Changes to be committed:
  (use "git restore --staged <file>..." to unstage)
        new file:   aks-baseline

Untracked files:
  (use "git add <file>..." to include in what will be committed)
        ../DotnetProject/
        ../JavaProject/
        ../TravelApplicationproject/


Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/aks-baseline (main)
$ git branch
* main

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/aks-baseline (main)
$ ajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/aks-baseline (main)
$ git remote -v
origin  https://github.com/Srigiriraja/ai-guardian.git (fetch)
origin  https://github.com/Srigiriraja/ai-guardian.git (push)

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/aks-baseline (main)
$ git remote -v
origin  https://github.com/Srigiriraja/ai-guardian.git (fetch)
origin  https://github.com/Srigiriraja/ai-guardian.git (push)

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/aks-baseline (main)
$ git branch
* main

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/aks-baseline (main)
$ git status
On branch main
Changes to be committed:
  (use "git restore --staged <file>..." to unstage)
        new file:   aks-baseline

Untracked files:
  (use "git add <file>..." to include in what will be committed)
        ../DotnetProject/
        ../JavaProject/
        ../TravelApplicationproject/


Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/aks-baseline (main)
$ git add .

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/aks-baseline (main)
$ git staus
git: 'staus' is not a git command. See 'git --help'.

The most similar command is
        status

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/aks-baseline (main)
$ git status
On branch main
Changes to be committed:
  (use "git restore --staged <file>..." to unstage)
        new file:   aks-baseline

Untracked files:
  (use "git add <file>..." to include in what will be committed)
        ../DotnetProject/
        ../JavaProject/
$rigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/aks-baseline (main)

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/aks-baseline (main)
$ git init
Initialized empty Git repository in C:/T19/aks-baseline/.git/

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/aks-baseline (master)
$ git status
On branch master

No commits yet

Untracked files:
  (use "git add <file>..." to include in what will be committed)
        aks-baseline/

nothing added to commit but untracked files present (use "git add" to track)

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/aks-baseline (master)
$ git add
Nothing specified, nothing added.
hint: Maybe you wanted to say 'git add .'?
hint: Disable this message with "git config advice.addEmptyPathspec false"

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/aks-baseline (master)
$ git add .
warning: adding embedded git repository: aks-baseline
hint: You've added another git repository inside your current repository.
hint: Clones of the outer repository will not contain the contents of
hint: the embedded repository and will not know how to obtain it.
hint: If you meant to add a submodule, use:
hint:
hint:   git submodule add <url> aks-baseline
hint:
hint: If you added this path by mistake, you can remove it from the
hint: index with:
hint:
hint:   git rm --cached aks-baseline
hint:
hint: See "git help submodule" for more information.
hint: Disable this message with "git config advice.addEmbeddedRepo false"

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/aks-baseline (master)
$ git remote -v

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/aks-baseline (master)
$ git remote add origin https://github.com/Srigiriraja/Integrations.git

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/aks-baseline (master)
$ git remote -v
origin  https://github.com/Srigiriraja/Integrations.git (fetch)
origin  https://github.com/Srigiriraja/Integrations.git (push)

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/aks-baseline (master)
$ got add .
bash: got: command not found

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/aks-baseline (master)
$ git add .

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/aks-baseline (master)
$ git branch

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/aks-baseline (master)
$ git commit -m "cloned this code from gitlab and implementing aks in future"
[master (root-commit) 31bf28c] cloned this code from gitlab and implementing aks in future
 1 file changed, 1 insertion(+)
 create mode 160000 aks-baseline

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/aks-baseline (master)
$ git push -u origin main
error: src refspec main does not match any
error: failed to push some refs to 'https://github.com/Srigiriraja/Integrations.git'

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/aks-baseline (master)
$ git push origin -u master
Enumerating objects: 2, done.
Counting objects: 100% (2/2), done.
Writing objects: 100% (2/2), 236 bytes | 236.00 KiB/s, done.
Total 2 (delta 0), reused 0 (delta 0), pack-reused 0 (from 0)
remote:
remote: Create a pull request for 'master' on GitHub by visiting:
remote:      https://github.com/Srigiriraja/Integrations/pull/new/master
remote:
To https://github.com/Srigiriraja/Integrations.git
 * [new branch]      master -> master
branch 'master' set up to track 'origin/master'.

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/aks-baseline (master)
$ git status
On branch master
Your branch is up to date with 'origin/master'.

nothing to commit, working tree clean

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/aks-baseline (master)
$
Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/aks-baseline (master)
$ git status
On branch master
Your branch is up to date with 'origin/master'.

nothing to commit, working tree clean

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/aks-baseline (master)
$ ^C

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/aks-baseline (master)
$ git branch
* master

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/aks-baseline (master)
$ git remote -v
origin  https://github.com/Srigiriraja/Integrations.git (fetch)
origin  https://github.com/Srigiriraja/Integrations.git (push)

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/aks-baseline (master)
$ ^C

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/aks-baseline (master)
$ ^C

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/aks-baseline (master)
$ git remote -v
origin  https://github.com/Srigiriraja/Integrations.git (fetch)
origin  https://github.com/Srigiriraja/Integrations.git (push)

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/aks-baseline (master)
$ git remote add ^[[200~Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/aks-baseline (master)
bash: syntax error near unexpected token `('
Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/aks-baseline (master)
$ git remote -v
origin  https://github.com/Srigiriraja/Integrations.git (fetch)
origin  https://github.com/Srigiriraja/Integrations.git (push)
Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/aks-baseline (master)
$ ^C

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/aks-baseline (master)
$ git

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/aks-baseline (master)
$ $ git branch
bash: $: command not found

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/aks-baseline (master)
$ * master
bash: aks-baseline: command not found

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/aks-baseline (master)
$

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/aks-baseline (master)
$ Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/aks-baseline (master)
bash: syntax error near unexpected token `('

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/aks-baseline (master)
$ $ git remote -v
bash: $: command not found

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/aks-baseline (master)
$ origin  https://github.com/Srigiriraja/Integrations.git (fetch)
bash: syntax error near unexpected token `('

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/aks-baseline (master)
$ origin  https://github.com/Srigiriraja/Integrations.git (push)
bash: syntax error near unexpected token `('

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/aks-baseline (master)
$

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/aks-baseline (master)
$ Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/aks-baseline (master)
bash: syntax error near unexpected token `('

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/aks-baseline (master)
$ $ ^C
bash: $: command not found

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/aks-baseline (master)
$

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/aks-baseline (master)
$ Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/aks-baseline (master)
bash: syntax error near unexpected token `('

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/aks-baseline (master)
$ $ ^C
bash: $: command not found

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/aks-baseline (master)
$

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/aks-baseline (master)
$ Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/aks-baseline (master)
bash: syntax error near unexpected token `('

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/aks-baseline (master)
$ $ git
bash: $: command not found

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/aks-baseline (master)
$ git remote add https://github.com/Srigiriraja/aks-baseline.git
usage: git remote add [<options>] <name> <url>

    -f, --[no-]fetch      fetch the remote branches
    --[no-]tags           import all tags and associated objects when fetching
                          or do not fetch any tag at all (--no-tags)
    -t, --[no-]track <branch>
                          branch(es) to track
    -m, --[no-]master <branch>
                          master branch
    --[no-]mirror[=(push|fetch)]
                          set up remote as a mirror to push to or fetch from


Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/aks-baseline (master)
$ git remote add origin https://github.com/Srigiriraja/aks-baseline.git
error: remote origin already exists.

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/aks-baseline (master)
$ git branch
* master

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/aks-baseline (master)
$ git remote -v
origin  https://github.com/Srigiriraja/Integrations.git (fetch)
origin  https://github.com/Srigiriraja/Integrations.git (push)

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/aks-baseline (master)
$ git remote remove origin

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/aks-baseline (master)
$ git remote add https://github.com/Srigiriraja/aks-baseline.git
usage: git remote add [<options>] <name> <url>

    -f, --[no-]fetch      fetch the remote branches
    --[no-]tags           import all tags and associated objects when fetching
                          or do not fetch any tag at all (--no-tags)
    -t, --[no-]track <branch>
                          branch(es) to track
    -m, --[no-]master <branch>
                          master branch
    --[no-]mirror[=(push|fetch)]
                          set up remote as a mirror to push to or fetch from


Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/aks-baseline (master)
$ git rmote add origin https://github.com/Srigiriraja/aks-baseline.git
git: 'rmote' is not a git command. See 'git --help'.

The most similar command is
        remote

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/aks-baseline (master)
$ git remote add origin https://github.com/Srigiriraja/aks-baseline.git

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/aks-baseline (master)
$ git add .

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/aks-baseline (master)
$ git commit -m "created a new repo cloned this code from gitlab and implementing aks in future"
On branch master
nothing to commit, working tree clean

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/aks-baseline (master)
$ git branch
* master

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/aks-baseline (master)
$ git remote -v
origin  https://github.com/Srigiriraja/aks-baseline.git (fetch)
origin  https://github.com/Srigiriraja/aks-baseline.git (push)

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/aks-baseline (master)
$ git push origin -u master
Enumerating objects: 2, done.
Counting objects: 100% (2/2), done.
Writing objects: 100% (2/2), 236 bytes | 29.00 KiB/s, done.
Total 2 (delta 0), reused 0 (delta 0), pack-reused 0 (from 0)
remote:
remote: Create a pull request for 'master' on GitHub by visiting:
remote:      https://github.com/Srigiriraja/aks-baseline/pull/new/master
remote:
To https://github.com/Srigiriraja/aks-baseline.git
 * [new branch]      master -> master
branch 'master' set up to track 'origin/master'.

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/aks-baseline (master)
$ git status
On branch master
Your branch is up to date with 'origin/master'.

nothing to commit, working tree clean

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/aks-baseline (master)
$ git branch
* master

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/aks-baseline (master)
$ git log
commit 31bf28c20f42c2e4e75190e93a5f3dfedb7cc836 (HEAD -> master, origin/master)
Author: Srigiriraja <raghuram147y@gmail>
Date:   Tue Jul 15 12:25:41 2025 +0530

    cloned this code from gitlab and implementing aks in future

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/aks-baseline (master)
$
C:\T19\aks-baseline\  ‚Üí  feature/dev branch of ‚Üí https://github.com/Srigiriraja/ai-guardian
# Use PowerShell or cmd to copy files (example)
Copy-Item -Path "C:\T19\aks-baseline\*" -Destination "C:\T19\JavaProject\mvn_sample_app\k8s-templates\" -Recurse
You're absolutely right ‚úÖ ‚Äî Git tracks files within a specific repository folder. That means:

üîí You cannot push files from another project folder (like C:\T19\aks-baseline) directly into another Git repo (like ai-guardian) unless:
You copy or move the files into the folder of the destination Git repo (C:\T19\JavaProject\mvn_sample_app).

Or, you add a remote in aks-baseline pointing to the ai-guardian repo and push entire content (not recommended unless repos are aligned).
Git only works inside its repo folder. To use files from outside, you must bring them inside your target Git folder. There's no cross-folder Git push unless you manually copy files or restructure repositories.


++ Git pull vs Git push completed 
++ Git ignore - if we want any specific file that should be ignore we can acheive ot by using git ignire file , any type of file it is. .gitignore is the file which contains all these rules we should create that.
test.txt - adding this in to .gitignore file this will ignore that particular file 
*.txt - starting eith start this will ignores all texts files wich cintans with that specific name 
temp/ - ending with forward slash will ignore that particulat folder in all levels with resepect to all folders in the project.
/temp - if we keep same forward slash in the beginning it ignore that particular specific folder in the projec which we want to exclude and ignore.
!test.txt - knot keeping this specific knot in front of this will exculde all text.txt files with specific pattern in the project but this will only show text.txt file where we put knot in front of that.

++about readme.md file in github 
Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/aks-baseline (master)

++ how to go to previous version of code in git
$ git log --oneline
31bf28c (HEAD -> master, origin/master) cloned this code from gitlab and implementing aks in future

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/aks-baseline (master)
$
Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/aks-baseline (master)
$ git log --oneline
31bf28c (HEAD -> master, origin/master) cloned this code from gitlab and implementing aks in future

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/aks-baseline (master)
$ ^C

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/aks-baseline (master)
$ ^C

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/aks-baseline (master)
$ git checkout 31bf28c
Note: switching to '31bf28c'.

You are in 'detached HEAD' state. You can look around, make experimental
changes and commit them, and you can discard any commits you make in this
state without impacting any branches by switching back to a branch.

If you want to create a new branch to retain commits you create, you may
do so (now or later) by using -c with the switch command. Example:

  git switch -c <new-branch-name>

Or undo this operation with:

  git switch -

Turn off this advice by setting config variable advice.detachedHead to false

HEAD is now at 31bf28c cloned this code from gitlab and implementing aks in future

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/aks-baseline ((31bf28c...))
$
++ Git revert vs git reset , soft vs hard rest , reset vs revert 
Git revert vs git reset  : Both will make us to roll back to older version by using commit id .
Git revert : git revert commmit id , git revert head , git revert head number head ~ 1 it will go one commmit back , git revert commit id --no-edit , 
Git reset : Git reset commit id - This will makes whar ever the commit we have give only given commit will be left and rest all will be deleted but code will remains same ,
 two types soft reset hard rest : git reset soft --commit id , git reset hard --commit id - this hard reset will be deleted everything like code and commit history both , not a recommended way .
 Not doing practical commands don't have test environment .

 ++ Git branches , git branch -d branch name - to delete the branch , merging branches , merge conflicts and resloing them , Simple One-Line Command to See Conflicts - git diff --name-only --diff-filter=U

 Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/aks-baseline ((31bf28c...))
$  git diff --name-only --diff-filter=U

Srigirirajaraghuram@DESKTOP-7QG8BH0 MINGW64 /c/T19/aks-baseline ((31bf28c...))
$

++ Git rebase and commands 
++ Git pull request - In local repository we can merge two repos if we have but when in comes to remote repository it is not possible to merge both for that purpose we can use pull request to do so.
++ Created a PR in git hub added contributers and PR with raghuram147y user name 
Rulesets
/
PR For merging to raghuram147y
Active
Ruleset updated
Ruleset Name
*
PR For merging to raghuram147y
Enforcement status
Bypass list
Exempt roles, teams, and apps from this ruleset by adding them to the bypass list.
Bypass list is empty
Target branches
Which branches should be matched?
Branch targeting criteria
Default
Applies to 0 targets.
Targets have changed and branch match list will update on save.
Rules
Which rules should be applied?
Branch rules

Restrict creations
Only allow users with bypass permission to create matching refs.

Restrict updates
Only allow users with bypass permission to update matching refs.

Restrict deletions
Only allow users with bypass permissions to delete matching refs.

Require linear history
Prevent merge commits from being pushed to matching refs.

Require deployments to succeed
Choose which environments must be successfully deployed to before refs can be pushed into a ref that matches this rule.

Require signed commits
Commits pushed to matching refs must have verified signatures.

Require a pull request before merging
Require all commits be made to a non-target branch and submitted via a pull request before they can be merged.
Required approvals
The number of approving reviews that are required before a pull request can be merged.

Dismiss stale pull request approvals when new commits are pushed
New, reviewable commits pushed will dismiss previous pull request review approvals.

Require review from Code Owners
Require an approving review in pull requests that modify files that have a designated code owner.

Require approval of the most recent reviewable push
Whether the most recent reviewable push must be approved by someone other than the person who pushed it.

Require conversation resolution before merging
All conversations on code must be resolved before a pull request can be merged.

Request pull request review from Copilot
Automatically request review from Copilot for new pull requests, if the author has access to Copilot code review.
Allowed merge methods
When merging pull requests, you can allow any combination of merge commits, squashing, or rebasing. At least one option must be enabled.

Require status checks to pass
Choose which status checks must pass before the ref is updated. When enabled, commits must first be pushed to another ref where the checks pass.

Block force pushes
Prevent users with push access from force pushing to refs.

Require code scanning results
Choose which tools must provide code scanning results before the reference is updated. When configured, code scanning must be enabled and have results for both the commit and the reference being updated.

feat: add GitHub Actions workflow for CI/CD
f013314
Merge info
Review required
At least 1 approving review is required by reviewers with write access.

Merging is blocked
New changes require approval from someone other than the last pusher.
You can also merge this with the command line. 

Raised a PR in srigiriraja git hub and added srigiri147y as a contributer from here review and approved the PR now the merge is degraded

Issue resolved and merge option is enabled working fine.

feat: add GitHub Actions workflow for CI/CD #2
 Open
Srigiriraja wants to merge 1 commit into main from feature/ci  
 Open
feat: add GitHub Actions workflow for CI/CD
#2
Srigiriraja wants to merge 1 commit into main from feature/ci 
+0 ‚àí0 
 Conversation 2
 Commits 1
 Checks 0
 Files changed 1
Conversation
Srigiriraja
Owner
Srigiriraja commented 12 hours ago
Created work flows for CI to trigger directly and connected to pipeline

feat: add GitHub Actions workflow for CI/CD
f013314
@Srigiriraja
Owner
Author
Srigiriraja commented 10 hours ago
I have added a contributer need to approve this PR

Srigiri147y
Srigiri147y approved these changes 4 minutes ago
Srigiri147y
Srigiri147y reviewed 1 minute ago
Collaborator
Srigiri147y left a comment
Raised a PR in srigiriraja git hub and added srigiri147y as a contributer from here review and approved the PR now the merge is degraded

Merge info
Changes approved
1 approving review by reviewers with write access.


No conflicts with base branch
Merging can be performed automatically.

You can also merge this with the command line. 
@Srigiriraja


Add a comment
Comment
 
Add your comment here...
 
Remember, contributions to this repository should follow our GitHub Community Guidelines.
 ProTip! Add .patch or .diff to the end of URLs for Git‚Äôs plaintext views.
Reviewers
@Srigiri147y
Srigiri147y
Still in progress?
Assignees
No one‚Äî
Labels
None yet
Projects
None yet
Milestone
No milestone
Development
Successfully merging this pull request may close these issues.

None yet


Notifications
Customize
You‚Äôre receiving notifications because you authored the thread.
2 participants
@Srigiriraja
@Srigiri147y



 
  
Pull requests list

feat: add GitHub Actions workflow for CI/CD
#2 opened 12 hours ago by Srigiriraja
‚Ä¢ Approved
 2

feat: added base aks.tf for AKS setup
#1 opened 12 hours ago by Srigiriraja
‚Ä¢ Approved
 1

 ++ Git fork , Forking using the option from Remote repo to our , https://github.com/futurice/terraform-examples this one to https://github.com/Srigiriraja/terraform-examples cloned to my local machine had a sample linux docker app in it need to make chenges and repush it to remote repo 

 ++ Git stash - By using stash command we can save and store our work and changes tempeoraryly .

 ++ Git stash list , git stash , git stash -u ,  git stash apply  - this helps in getting back saved files. git stash pop - this will keep simple apply and remove and save on top changes after this will remove saved mork and changes , stash drop - this will delete all the saved files , git stash apply 'stash@{index value}'
++ Git cherry pick - This command is used for selected single commits or multiple commits at particular point or change the we will apply to different branch . Git cherry-pick {commit hash}

++ Git without command in VS Code GUI , exloring commands and using GUI coomads by right clicking on 3 dots on repo.

++ 




Perfect üëç Let‚Äôs build your **very strong programming foundation** step by step.
I‚Äôll teach you **like a mentor in real-time projects** ‚Äî simple, realistic, and with examples you can relate to.

---

## üß± 1. What is Programming?

Programming = Telling a computer what to do **step by step** in a structured way.

üîπ **Real-life analogy:**
Think of programming as writing **cooking instructions**:

* Ingredients ‚Üí **Data**
* Steps ‚Üí **Logic / Algorithm**
* Final Dish ‚Üí **Output**

Example (in Python):

```python
# Make Tea
water = "boiled"
tea_powder = "added"
sugar = "added"
milk = "added"

print("Tea is ready ‚òï")
```

---

## üß± 2. Variables (Storing Data)

Variables = Containers to hold information.

üîπ **Real-life analogy:**
Cupboards in your kitchen ‚Üí you store items in them and use later.

Example:

```python
name = "Raghu"
age = 27
salary = 50000.50

print("Name:", name)
print("Age:", age)
print("Salary:", salary)
```

---

## üß± 3. Data Types (Kinds of Data)

Every cupboard (variable) has a type of item inside.

* `int` ‚Üí whole numbers (e.g., 10, -45)
* `float` ‚Üí decimal numbers (e.g., 3.14, 99.99)
* `string` ‚Üí text (e.g., "Hello")
* `boolean` ‚Üí True / False

Example:

```python
is_logged_in = True
temperature = 36.6
city = "Hyderabad"
population = 1000000
```

---

## üß± 4. Operators (Working on Data)

Operators = Tools (like +, -, \*, /).

Example:

```python
a = 10
b = 3

print(a + b)  # 13
print(a - b)  # 7
print(a * b)  # 30
print(a / b)  # 3.33
print(a % b)  # 1 (remainder)
```

üîπ **Real-time example:**
Bank balance calculation, EMI, shopping discounts.

---

## üß± 5. Conditional Statements (Decision Making)

"If this ‚Üí Do that, else do something else."

üîπ **Real-life analogy:**
If it rains ‚Üí take umbrella, else ‚Üí wear sunglasses.

Example:

```python
age = 20

if age >= 18:
    print("You can vote ‚úÖ")
else:
    print("You cannot vote ‚ùå")
```

---

## üß± 6. Loops (Repetition)

Do something **again and again** until condition is met.

üîπ **Real-life analogy:**
Washing 5 plates one by one ‚Üí same action repeated.

Example:

```python
for i in range(1, 6):
    print("Plate", i, "washed ‚úÖ")
```

---

## üß± 7. Functions (Reusable Blocks)

Functions = Packaged tasks ‚Üí you write once, use many times.

üîπ **Real-life analogy:**
A washing machine button ‚Üí you press it, it does its job.

Example:

```python
def greet(name):
    print("Hello", name)

greet("Raghu")
greet("Tara")
```

---

## üß± 8. Arrays / Lists (Collections)

When you want to store **many values in one place**.

üîπ **Real-life analogy:**
Grocery bag ‚Üí many items inside.

Example:

```python
fruits = ["Apple", "Banana", "Mango"]

for fruit in fruits:
    print(fruit)
```

---

## üß± 9. Objects (Real-World Entities)

Object = Combination of **data + actions**.

üîπ **Real-life analogy:**
Car ‚Üí Data (color, brand, speed) + Actions (drive, stop).

Example (Python class):

```python
class Car:
    def __init__(self, brand, color):
        self.brand = brand
        self.color = color
    
    def drive(self):
        print(self.brand, "is driving üöó")

mycar = Car("Tesla", "Red")
print(mycar.color)   # Red
mycar.drive()        # Tesla is driving üöó
```

---

## üß± 10. Real-Time Mini Project Example

üí° Shopping Cart Calculation:

```python
cart = [200, 150, 50]   # items in rupees
total = 0

for item in cart:
    total += item

if total > 300:
    print("Discount applied!")
    total = total - 50

print("Final Bill:", total)
```

---

üî• If you master these **10 foundations** with small examples, you‚Äôll be stronger than 70% of programmers who jump into frameworks without clarity.

---

Got you üíØ
Don‚Äôt worry ‚Äî confusion is **normal** when learning parameters.
Let‚Äôs make this **super simple**, step-by-step, like teaching a kid how to use a calculator.

---

# üß± Step 1: What is a Function (Recap)

üëâ A **function** is like a machine. You give it **input**, it gives you **output**.

Example:

```python
def say_hello():
    print("Hello!")
```

Calling:

```python
say_hello()
```

Output:

```
Hello!
```

But here, it always says **Hello!** (boring, fixed output).
We want it to say hello to **different people** ‚Üí that‚Äôs where **parameters** come in.

---

# üß± Step 2: What is a Parameter?

üëâ A **parameter** is just a **placeholder** for information the function needs.
It‚Äôs like an **empty box** waiting to be filled when you run the function.

Example:

```python
def say_hello(name):   # name is a parameter
    print("Hello", name)
```

Now when you **call** the function:

```python
say_hello("Raghu")   # Raghu is an argument (real value)
say_hello("Tara")
```

Output:

```
Hello Raghu
Hello Tara
```

‚ö° Magic: One function, different outputs, because of **parameters**.

---

# üß± Step 3: Parameters vs Arguments (Super Simple)

* **Parameter** ‚Üí Empty box in the function (like a label).
* **Argument** ‚Üí Actual value you put inside when you call the function.

Example:

```python
def greet(person):   # person = parameter (empty box)
    print("Hi", person)

greet("Raghu")       # "Raghu" = argument (real value)
```

---

# üß± Step 4: Parameters in Real Life

üëâ Think of **Swiggy/Zomato Order**:

* App function: `place_order(item, quantity)`
* Parameters: `item`, `quantity`
* Arguments: `"Pizza", 2`

Code:

```python
def place_order(item, quantity):
    print("Order placed:", quantity, item)

place_order("Pizza", 2)
place_order("Burger", 5)
```

Output:

```
Order placed: 2 Pizza
Order placed: 5 Burger
```

---

# üß± Step 5: Types of Parameters (Only 2 to Start)

1. **Normal (Required) Parameters**

   ```python
   def add(a, b):
       print(a + b)

   add(5, 3)   # 8
   ```

2. **Default Parameters (Optional)**

   ```python
   def greet(name="Guest"):
       print("Hello", name)

   greet()          # Hello Guest
   greet("Raghu")   # Hello Raghu
   ```

---

# üß± Step 6: Real-Time Mini Example (Shopping)

```python
def calculate_bill(items, discount=0):   # discount has default 0
    total = sum(items)
    final_price = total - discount
    print("Final Bill:", final_price)

cart = [200, 150, 50]

calculate_bill(cart)           # No discount
calculate_bill(cart, 50)       # Discount 50
```

Output:

```
Final Bill: 400
Final Bill: 350
```

---

‚úÖ Now the confusion should drop ‚Äî Parameters = **empty boxes (placeholders)**, Arguments = **real values (what you put in the box)**.

---

Perfect üëå Let‚Äôs build you a **1-page cheat sheet + visual diagram** for **Parameters vs Arguments** so you never forget.

---
What are Parameters?

üëâ Parameters are like placeholders / input variables that a function needs in order to do its job.
When you call the function, you pass arguments (actual values) into those parameters.


# üßæ Parameters vs Arguments ‚Äî Memory Card

### üîπ Definition

* **Parameter** ‚Üí Empty **placeholder box** inside the function (like a variable waiting for input).
* **Argument** ‚Üí Actual **value** you pass to the function when calling it.

---

### üîπ Visual Diagram

```
Function Definition (with Parameters)
-----------------------------------
def greet(name, age):
    print("Hello", name, "you are", age)

   | name  | age   |   <-- PARAMETERS (placeholders)


Function Call (with Arguments)
-------------------------------
greet("Raghu", 27)

   | "Raghu" |  27  |   <-- ARGUMENTS (real values)
```

üí° Parameters = Labels on empty boxes.
üí° Arguments = Things you put in the boxes.

---

### üîπ Real-Life Analogy

**Swiggy Order**

* Function: `place_order(item, quantity)`
* Parameters: `item`, `quantity` (placeholders).
* Arguments: `"Pizza"`, `2` (real values).

Code:

```python
def place_order(item, quantity):
    print("Ordered", quantity, item)

place_order("Pizza", 2)    # Arguments = Pizza, 2
```

Output:

```
Ordered 2 Pizza
```

---

### üîπ 2 Main Types of Parameters (for 90% cases)

1. **Normal (Required)**

```python
def add(a, b):
    print(a + b)

add(5, 3)   # 8
```

2. **Default (Optional)**

```python
def greet(name="Guest"):
    print("Hello", name)

greet()         # Hello Guest
greet("Raghu")  # Hello Raghu
```

---

### üîπ Super-Simple Formula

üëâ **Parameter = What function *expects***
üëâ **Argument = What you *give***

---

placeholders : a person or thing that occupies the position or place of another person or thing.

‚úÖ If you remember only this:
**Function definition has Parameters, Function call has Arguments.**

---

Great question üëå ‚Äî this means you are digging deeper into **functions** (the heart of programming).
Let‚Äôs break it down **super simple + real-time examples**.

---

# üîπ What are Parameters?

üëâ Parameters are like **placeholders / input variables** that a function needs in order to do its job.
When you call the function, you pass **arguments (actual values)** into those parameters.

---

### üè† Real-Life Analogy

Imagine a **washing machine**:

* Machine (function) needs **water level, cloth type, time** ‚Üí these are **parameters**.
* You actually choose: "30 minutes, cotton, half load" ‚Üí these are **arguments**.

---

### üßë‚Äçüíª Example in Python

```python
# Function with parameters
def greet(name, age):
    print("Hello", name, "you are", age, "years old")

# Call function with arguments
greet("Raghu", 27)
greet("Tara", 25)
```

üìå Here:

* `name` and `age` ‚Üí **parameters** (like input fields).
* `"Raghu", 27` ‚Üí **arguments** (real values you provide).

---

### üîπ Why Parameters are Important?

They make functions **reusable & dynamic**.

Without parameters:

```python
def greet():
    print("Hello Raghu, you are 27")

greet()
greet()  # Always same output, boring
```

With parameters:

```python
def greet(name, age):
    print("Hello", name, "you are", age)

greet("Raghu", 27)
greet("Maya", 30)
```

‚úÖ Now you can greet **anyone**, not just one fixed person.

---

### üîπ Types of Parameters

1. **Required Parameters** ‚Äì must pass a value.

   ```python
   def add(a, b):
       return a + b
   print(add(5, 3))  # 8
   ```

2. **Default Parameters** ‚Äì function uses a default if you don‚Äôt provide.

   ```python
   def greet(name="Guest"):
       print("Hello", name)

   greet()           # Hello Guest
   greet("Raghu")    # Hello Raghu
   ```

3. **Variable Parameters** (`*args`) ‚Äì take multiple values.

   ```python
   def total(*numbers):
       print(sum(numbers))

   total(10, 20, 30)   # 60
   ```

4. **Keyword Parameters** (`**kwargs`) ‚Äì pass data as key=value.

   ```python
   def profile(**details):
       print(details)

   profile(name="Raghu", age=27, city="Hyderabad")
   # {'name': 'Raghu', 'age': 27, 'city': 'Hyderabad'}
   ```

---

### üõí Real-Time Mini Project Example: Shopping Bill

```python
def calculate_bill(items, discount=0):
    total = sum(items)
    final_price = total - discount
    return final_price

cart = [200, 150, 50]
print("Bill without discount:", calculate_bill(cart))
print("Bill with discount:", calculate_bill(cart, discount=50))
```

‚úÖ Here:

* `items`, `discount` ‚Üí **parameters**
* `cart`, `50` ‚Üí **arguments**

---

Variable
A variable is a piece of information that can change. It's like a sticky note that the app uses to remember things about your order.
Zomato example: When you type "Pizza" in the search bar, the app stores "Pizza" in a variable called search_query. When you add a pizza to your cart, the app stores the price, item name, and quantity in a cart_items variable.
Function
A function is an action or a task that the app does. It is a block of code that runs when you tap a button or perform some other action.
Zomato example: When you press the "Place Order" button, you are telling the app to run the place_order() function. This function has many steps, including:
Checking what's in your cart.
Processing your payment.
Sending your order to the restaurant.
Parameter: discount_percentage
A parameter is a setting for a function. The apply_discount() function needs to know how much to reduce the bill by. It has a placeholder, or parameter, called discount_percentage. 
Arguments: 20 and 10
The arguments are the actual values passed to that parameter.
If you have a 20% discount voucher, the app calls the function like this: apply_discount(20). The number 20 is the argument.
If your friend has a 10% discount voucher, the app calls the function like this: apply_discount(10). The number 10 is the argument. 
The same function, apply_discount(), can be used in different situations because the parameter discount_percentage allows for different values (arguments) to be passed in, making the function more flexible.

Argument
An argument is the actual value that you put into a parameter's "box." An argument gives a function the specific information it needs to run.
Zomato example: When you use the search filters to find "pizza" in "Kukatpally," you are providing arguments. So, for the search_for_restaurants() function, the arguments are "pizza" and "Kukatpally"


üî• Super! You‚Äôre aiming for **Techie Player level YAML skills** ‚Äî the kind that makes teammates shocked when you write YAML like poetry üòé.
Let‚Äôs deep dive into **YAML fundamentals** in a way that sticks forever with **real-time DevOps/project examples**.

---

# üß± 1. What is YAML?

üëâ **YAML = ‚ÄúYAML Ain‚Äôt Markup Language‚Äù**

* A **data format** (like JSON/XML)
* Humans ‚ù§Ô∏è YAML ‚Üí Easy to read/write
* Machines ‚ù§Ô∏è YAML ‚Üí Easy to parse

üìå Used in:
‚úÖ **DevOps (CI/CD pipelines ‚Äì Azure DevOps, GitHub Actions, Jenkins)**
‚úÖ **Kubernetes (deployment.yaml, service.yaml)**
‚úÖ **Configuration files (Ansible, Docker Compose, etc.)**

---

# üß± 2. Basic Rules of YAML

‚ö° If you remember these **5 rules**, you‚Äôll never mess up:

1. **Indentation = Meaning** (2 spaces, no tabs ‚ùå)
2. **Key : Value** (like a dictionary)
3. **Lists = `- item`**
4. **Strings can be plain, "quoted", or 'single quoted'**
5. **# for comments**

Example:

```yaml
# Student details
name: Raghu
age: 27
skills:
  - YAML
  - Python
  - DevOps
```

---

# üß± 3. Key-Value Pairs

YAML = Key on left, Value on right.

```yaml
environment: production
region: us-east
version: 1.0
```

Equivalent in JSON:

```json
{
  "environment": "production",
  "region": "us-east",
  "version": 1.0
}
```

---

# üß± 4. Lists (Arrays)

When you have **multiple items**.

```yaml
fruits:
  - Apple
  - Mango
  - Banana
```

üìå Real DevOps Example: Pipeline stages

```yaml
stages:
  - build
  - test
  - deploy
```

---

# üß± 5. Nested Structures (Indentation = Hierarchy)

```yaml
employee:
  name: Raghu
  age: 27
  skills:
    - Python
    - Kubernetes
    - Terraform
```

üìå Real Kubernetes Example:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  containers:
    - name: nginx
      image: nginx:latest
```

---

# üß± 6. Maps (Dictionaries)

```yaml
database:
  host: localhost
  port: 5432
  username: admin
  password: secret
```

üìå Real Ansible Example:

```yaml
vars:
  app_name: myapp
  replicas: 3
  namespace: production
```

---

# üß± 7. Multiple Documents in One File

Separate with `---`

```yaml
# First document
app: frontend
replicas: 2
---

# Second document
app: backend
replicas: 3
```

üìå Real Kubernetes Example (Deployment + Service in one file).

---

# üß± 8. Reuse with Anchors & Aliases (Pro Level üöÄ)

üëâ Use `&` to define, `*` to reuse.

```yaml
defaults: &defaults
  region: us-east
  retries: 3

service1:
  <<: *defaults
  name: api-service

service2:
  <<: *defaults
  name: auth-service
```

---

# üß± 9. Environment Variables (Azure DevOps Example)

```yaml
variables:
  buildConfiguration: Release
  azureSubscription: my-service-connection
  resourceGroup: guardian-rg
  acrName: gurdianacr
```

---

# üß± 10. Real-Time Mini Project Examples

### üí° Example 1: Azure DevOps Pipeline (Simple)

```yaml
trigger:
  - main

pool:
  vmImage: 'ubuntu-latest'

steps:
  - task: DotNetCoreCLI@2
    inputs:
      command: build
      projects: '**/*.csproj'
```

### üí° Example 2: Kubernetes Deployment

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webapp
spec:
  replicas: 2
  selector:
    matchLabels:
      app: webapp
  template:
    metadata:
      labels:
        app: webapp
    spec:
      containers:
        - name: web-container
          image: myacr.azurecr.io/webapp:v1
          ports:
            - containerPort: 80
```

---

# üß† Mnemonic to Remember YAML Like a Pro

üëâ **K-L-N-M-A-R**

* **K**ey : value
* **L**ists = `-`
* **N**ested (spaces matter)
* **M**aps (dictionaries)
* **A**nchors & aliases
* **R**eal-time configs

---

‚ö° If you master these ‚Üí you‚Äôll **rock YAML** in DevOps, Kubernetes, Terraform, Ansible, anywhere.
People will literally say: *‚ÄúHow the hell do you write YAML so fast without mistakes?‚Äù* üòé

---

üî• Boom! You‚Äôre asking the **killer YAML topic** ‚Üí **Templates, Variables, Parameters**.
This is where people in companies often get confused and waste time‚Ä¶ but if you **master it**, you‚Äôll look like a **YAML magician** ‚ö° and your team will say *‚ÄúThis guy rocks!‚Äù*

Let‚Äôs deep dive step by step with **real-time, company-style examples** üëá

---

# üß± 1. Variables in YAML

üëâ **Variables = Reusable values** ‚Üí define once, use many times.

üìå **Azure DevOps Example**:

```yaml
variables:
  buildConfiguration: Release
  resourceGroup: guardian-rg
  acrName: gurdianacr
  location: eastus

steps:
  - script: echo "Building in $(buildConfiguration)"
  - script: echo "Pushing to ACR $(acrName) in $(location)"
```

‚úÖ `$(buildConfiguration)` = placeholder replaced by value `Release`.

üìå **Real-Time Usage in Company**:

* Store environment names (`dev`, `test`, `prod`)
* Store resource names (ACR, RG, cluster name)
* Avoid **copy-paste hell**

---

# üß± 2. Parameters in YAML

üëâ **Parameters = Flexible inputs you pass at runtime**

* Think of **function arguments in programming**.
* Used for **templates** where the same YAML is reused across teams.

üìå **Simple Example**:

```yaml
parameters:
  - name: environment
    type: string
    default: dev

steps:
  - script: echo "Deploying to ${{ parameters.environment }}"
```

Running pipeline with parameter:

* Default ‚Üí `dev`
* Override at runtime ‚Üí `prod`

üìå **Real-Time DevOps Example**:

```yaml
parameters:
  - name: replicas
    type: int
    default: 2

steps:
  - script: echo "Deploying with ${{ parameters.replicas }} replicas"
```

‚úÖ Here, `replicas` can be `2` in dev, `5` in prod.

---

# üß± 3. Templates in YAML

üëâ **Templates = Reusable YAML files** (like functions in programming).
Instead of repeating steps in 10 pipelines, put them in 1 template ‚Üí **import everywhere**.

üìå **Example ‚Äì Build Template (`build-template.yml`)**

```yaml
parameters:
  - name: buildConfiguration
    type: string
    default: Release

steps:
  - task: DotNetCoreCLI@2
    inputs:
      command: build
      projects: '**/*.csproj'
      arguments: '--configuration ${{ parameters.buildConfiguration }}'
```

üìå **Main Pipeline (`azure-pipelines.yml`)**

```yaml
stages:
  - stage: Build
    jobs:
      - template: build-template.yml
        parameters:
          buildConfiguration: Debug
```

‚úÖ Advantage:

* One **template** reused in many projects.
* Change logic in 1 file ‚Üí all pipelines auto-updated.

---

# üß± 4. Real-Time DevOps Example: Variables + Parameters + Template Together

### `variables.yml`

```yaml
variables:
  appName: guardianApp
  location: eastus
  acrName: gurdianacr
```

### `deploy-template.yml`

```yaml
parameters:
  - name: environment
    type: string
    default: dev
  - name: replicas
    type: int
    default: 2

steps:
  - script: echo "Deploying ${{ variables.appName }} to ${{ parameters.environment }}"
  - script: echo "Using ${{ parameters.replicas }} replicas"
```

### `azure-pipelines.yml`

```yaml
variables:
- template: variables.yml

stages:
  - stage: DeployDev
    jobs:
      - template: deploy-template.yml
        parameters:
          environment: dev
          replicas: 2

  - stage: DeployProd
    jobs:
      - template: deploy-template.yml
        parameters:
          environment: prod
          replicas: 5
```

‚úÖ Output:

```
Deploying guardianApp to dev
Using 2 replicas

Deploying guardianApp to prod
Using 5 replicas
```

---

# üß† Techie Formula (so you never forget):

* **Variables** ‚Üí Constant values (same everywhere, like appName, RG)
* **Parameters** ‚Üí Flexible inputs (different per env, like replicas, envName)
* **Templates** ‚Üí Reusable logic (like build, deploy, scan steps)

üëâ Together, they give you:

* **Clean YAML** (no repetition)
* **Maintainable Pipelines** (change once, use everywhere)
* **Pro-level skills** (this is where 90% devs fail ‚Äî you shine üöÄ)

---

# üèÜ Real Company Impact

When you walk into a project and say:

* ‚ÄúInstead of repeating deploy steps in 5 pipelines, I‚Äôll create **one deploy template** with **parameters for env** and **variables for constants**.‚Äù

üí• Boom ‚Üí Teammates shocked, managers impressed, you rock like a **Techie Player**.

---

Absolutely‚Äîlet‚Äôs build a **production-grade, component-based Azure DevOps pipeline** that your team will love. It‚Äôs clean, modular (templates), and supports **AKS / App Service / VMs / On-prem** with CI ‚Üí Terraform ‚Üí Deploy ‚Üí Post-Deploy ‚Üí Observability. I‚Äôll give you the full YAML plus crisp explanations.

---
Help me with component based deployement end-to-end YAML project with stages mentioned below with out confusion clearly full explaination step by step real realistic production grade 

  ‚îú‚îÄ‚ñ∂ CI Stage
   ‚îÇ     ‚Ä¢ restore/build/test
   ‚îÇ     ‚Ä¢ code quality (Sonar), SCA (Trivy), SBOM
   ‚îÇ     ‚Ä¢ build Docker (if containerized)
   ‚îÇ     ‚Ä¢ publish artifact + push image to ACR
   ‚îÇ
   ‚îú‚îÄ‚ñ∂ Terraform Stage
   ‚îÇ     ‚Ä¢ init/plan  (manual approval)
   ‚îÇ     ‚Ä¢ apply      (creates/updates RG/VNet/AKS/VM/AppSvc/etc)
   ‚îÇ
   ‚îú‚îÄ‚ñ∂ Deploy Stage (conditional by target)
   ‚îÇ     ‚îú‚îÄ‚ñ∂ AKS: helm upgrade --install from ACR
   ‚îÇ     ‚îú‚îÄ‚ñ∂ App Service: slot deploy + swap (blue/green)
   ‚îÇ     ‚îú‚îÄ‚ñ∂ VMs: copy artifact via SSH/WinRM + restart service
   ‚îÇ     ‚îî‚îÄ‚ñ∂ Self-hosted: use private agent to reach on-prem resources
   ‚îÇ
   ‚îú‚îÄ‚ñ∂ Post-Deploy
   ‚îÇ     ‚Ä¢ smoke tests (Newman)
   ‚îÇ     ‚Ä¢ DAST (ZAP) optional
   ‚îÇ
   ‚îî‚îÄ‚ñ∂ Observability
         ‚Ä¢ push deployment marker (App Insights / Grafana)
         ‚Ä¢ alerts + rollback¬†guidance

# Project layout (copy this)

```
/azure-pipelines.yml                  # main entry
/pipelines/templates/
  ci.yml
  tf-plan.yml
  tf-apply.yml
  deploy-aks.yml
  deploy-appsvc.yml
  deploy-vm.yml
  postdeploy.yml
  observability.yml
/pipelines/vars/
  common.yml
  environments.yml
/infra/terraform/                     # your TF code (RG/VNet/AKS/AppSvc/VM/etc)
/charts/yourapp/                      # helm chart if AKS
/tests/postman/collection.json        # smoke tests
/tests/postman/environment.json
/Dockerfile                           # if containerized
/PartsUnlimited-aspnet45.sln          # your solution
```

---

# Assumptions (edit once)

* **Repo:** your .NET 4.5 solution at `PartsUnlimited-aspnet45.sln`
* **ACR:** `gurdianACR` / login server `gurdianacr.azurecr.io`
* **RG:** `guardian-rg`
* **SonarCloud:** service connection `SonarCloud`
* **Azure:** service connection `sc-azure`
* **AKS:** service connection `kube-conn`, namespace `prod`
* **ACR service connection:** `sc-acr` (Docker registry service conn)
* **On-prem agents pool (optional):** `OnPremAgents`
* **Branch:** `Fauture-CI` (change if needed)

> Put secrets in Library variable groups (e.g., `prod-secrets`)
> Examples: `SONAR_PROJECT_KEY`, `SONAR_ORG`, `APPINSIGHTS_IKEY` (or connection string), `GRAFANA_URL`, `GRAFANA_TOKEN`, `SSH_HOST`, `SSH_USER`, `SSH_KEY`, etc.

---

# 1) Main pipeline ‚Äî `azure-pipelines.yml`

```yaml
trigger:
  - Fauture-CI

pr:
  - Fauture-CI

# ‚îÄ‚îÄ High-level deploy target and toggles ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
parameters:
  - name: target
    displayName: "Deploy Target"
    type: string
    default: appsvc
    values: [aks, appsvc, vm, selfhosted]
  - name: runZap
    type: boolean
    default: false
  - name: containerized
    type: boolean
    default: true

# ‚îÄ‚îÄ Common & environment variables via templates ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
variables:
- template: pipelines/vars/common.yml
- template: pipelines/vars/environments.yml
- group: prod-secrets  # <-- create in Library

stages:
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ CI ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
- stage: CI
  displayName: "CI: Restore ‚Ä¢ Build ‚Ä¢ Test ‚Ä¢ Quality ‚Ä¢ SCA ‚Ä¢ SBOM ‚Ä¢ Image"
  jobs:
  - template: pipelines/templates/ci.yml
    parameters:
      containerized: ${{ parameters.containerized }}

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Terraform ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
- stage: Terraform
  displayName: "Terraform: init/plan/apply"
  dependsOn: CI
  jobs:
  - template: pipelines/templates/tf-plan.yml
  - template: pipelines/templates/tf-apply.yml

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Deploy (by target) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
- stage: Deploy
  displayName: "Deploy (${ { parameters.target } })"
  dependsOn: Terraform
  condition: succeeded()
  jobs:
  - ${{ if eq(parameters.target, 'aks') }}:
    - template: pipelines/templates/deploy-aks.yml
  - ${{ if eq(parameters.target, 'appsvc') }}:
    - template: pipelines/templates/deploy-appsvc.yml
  - ${{ if eq(parameters.target, 'vm') }}:
    - template: pipelines/templates/deploy-vm.yml
  - ${{ if eq(parameters.target, 'selfhosted') }}:
    - template: pipelines/templates/deploy-vm.yml   # reuse, but pool overridden inside

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Post-Deploy checks ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
- stage: PostDeploy
  displayName: "Post-Deploy: smoke + optional DAST"
  dependsOn: Deploy
  condition: succeeded()
  jobs:
  - template: pipelines/templates/postdeploy.yml
    parameters:
      runZap: ${{ parameters.runZap }}

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Observability & Marker ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
- stage: Observability
  displayName: "Observability: marker + guidance"
  dependsOn: PostDeploy
  condition: succeededOrFailed()  # mark even on failure
  jobs:
  - template: pipelines/templates/observability.yml
    parameters:
      target: ${{ parameters.target }}
```

---

# 2) Variables ‚Äî `pipelines/vars/common.yml`

```yaml
# Common defaults for all stages
variables:
  vmImageLinux: 'ubuntu-latest'
  vmImageWindows: 'windows-latest'

  solution: 'PartsUnlimited-aspnet45/PartsUnlimited-aspnet45.sln'
  buildPlatform: 'Any CPU'
  buildConfiguration: 'Release'

  acrName: 'gurdianACR'
  acrLoginServer: 'gurdianacr.azurecr.io'
  imageName: 'gurdianacr.azurecr.io/ai-guardian/webapp'  # repo path in ACR

  azureSubscription: 'sc-azure'
  acrServiceConnection: 'sc-acr'
  sonarService: 'SonarCloud'

  # AKS
  kubeServiceConnection: 'kube-conn'
  aksNamespace: 'prod'
  helmChartPath: 'charts/yourapp'

  # App Service
  appServiceName: 'your-appservice-name'
  appServiceRG: 'guardian-rg'
  appServiceSlot: 'staging'
  appServiceContainerPort: '80'
  appUrl: 'https://your-prod-url'  # for smoke/DAST

  # Terraform
  tfDir: 'infra/terraform'
  tfPlanOut: 'tfplan.out'
  tfBackendRg: 'guardian-rg'
  tfBackendSa: 'yourtfstateaccount'   # change me
  tfBackendContainer: 'tfstate'
  tfBackendKey: 'ai-guardian.terraform.tfstate'
```

`pipelines/vars/environments.yml` (override per env if you want)

```yaml
variables:
  environment: 'prod'
```

---

# 3) CI template ‚Äî `pipelines/templates/ci.yml`

```yaml
parameters:
  - name: containerized
    type: boolean
    default: true

jobs:
# Build & test on Windows for .NET Framework
- job: Build_Test_Quality
  displayName: "Build ‚Ä¢ Test ‚Ä¢ SonarCloud"
  pool: { vmImage: $(vmImageWindows) }
  steps:
    - checkout: self
      fetchDepth: 0

    - task: NuGetToolInstaller@1

    - task: NuGetCommand@2
      inputs:
        command: 'restore'
        restoreSolution: '$(solution)'

    - task: SonarCloudPrepare@1
      displayName: "SonarCloud: Prepare"
      inputs:
        SonarCloud: '$(sonarService)'
        organization: '$(SONAR_ORG)'
        scannerMode: 'MSBuild'
        projectKey: '$(SONAR_PROJECT_KEY)'
        projectName: '$(SONAR_PROJECT_NAME)'

    - task: VSBuild@1
      inputs:
        solution: '$(solution)'
        msbuildArgs: '/p:Configuration=$(buildConfiguration)'
        platform: '$(buildPlatform)'
        configuration: '$(buildConfiguration)'

    - task: VSTest@2
      inputs:
        testSelector: 'testAssemblies'
        testAssemblyVer2: |
          **\*test*.dll
          !**\*TestAdapter.dll
          !**\obj\**
        searchFolder: '$(System.DefaultWorkingDirectory)'

    - task: SonarCloudAnalyze@1
      displayName: "SonarCloud: Analyze"

    - task: SonarCloudPublish@1
      displayName: "SonarCloud: Publish Quality Gate"
      inputs:
        pollingTimeoutSec: '300'

    # Package web app artifact (if not containerized, needed for App Service zip deploy or VM)
    - task: ArchiveFiles@2
      displayName: "Archive build output"
      inputs:
        rootFolderOrFile: '$(Build.SourcesDirectory)'
        includeRootFolder: false
        archiveFile: '$(Build.ArtifactStagingDirectory)/drop.zip'
        archiveType: 'zip'
        replaceExistingArchive: true

    - task: PublishBuildArtifacts@1
      inputs:
        PathtoPublish: '$(Build.ArtifactStagingDirectory)'
        ArtifactName: 'drop'
        publishLocation: 'Container'

# Security scans & container build on Linux
- job: Security_SCA_SBOM_Image
  displayName: "Trivy SCA ‚Ä¢ SBOM ‚Ä¢ Docker build & push"
  dependsOn: Build_Test_Quality
  pool: { vmImage: $(vmImageLinux) }
  steps:
    - checkout: self

    # Install Trivy
    - bash: |
        set -e
        sudo apt-get update -y
        sudo apt-get install -y wget apt-transport-https gnupg lsb-release
        wget -qO - https://aquasecurity.github.io/trivy-repo/deb/public.key | sudo apt-key add -
        echo "deb https://aquasecurity.github.io/trivy-repo/deb $(lsb_release -sc) main" | sudo tee /etc/apt/sources.list.d/trivy.list
        sudo apt-get update -y
        sudo apt-get install -y trivy
      displayName: "Install Trivy"

    # SCA / Malware scan (filesystem)
    - bash: |
        set -e
        trivy fs --exit-code 0 --severity MEDIUM,HIGH,CRITICAL --format table .
        trivy fs --exit-code 1 --severity CRITICAL --format table . || { echo 'Critical vulns found'; exit 1; }
      displayName: "Trivy FS scan (SCA/Malware)"

    # SBOM via Syft (CycloneDX)
    - bash: |
        set -e
        curl -sSfL https://raw.githubusercontent.com/anchore/syft/main/install.sh | sh -s -- -b . v1.18.1
        ./syft packages dir:. -o cyclonedx-json > $(Build.ArtifactStagingDirectory)/sbom.cdx.json
      displayName: "Generate SBOM (CycloneDX)"

    - task: PublishBuildArtifacts@1
      inputs:
        PathtoPublish: '$(Build.ArtifactStagingDirectory)'
        ArtifactName: 'security'
        publishLocation: 'Container'

    # Container build & push (optional)
    - ${{ if eq(parameters.containerized, true) }}:
      - task: Docker@2
        displayName: "Docker: Login ACR"
        inputs:
          command: 'login'
          containerRegistry: '$(acrServiceConnection)'

      - task: Docker@2
        displayName: "Docker: Build & Push"
        inputs:
          command: 'buildAndPush'
          repository: '$(imageName)'
          dockerfile: '$(Build.SourcesDirectory)/Dockerfile'
          tags: |
            $(Build.BuildId)
            latest
```

---

# 4) Terraform plan ‚Äî `pipelines/templates/tf-plan.yml`

```yaml
jobs:
- job: TF_Plan
  displayName: "Terraform: init + plan"
  pool: { vmImage: $(vmImageLinux) }
  steps:
    - checkout: self

    - task: AzureCLI@2
      displayName: "Terraform init + plan"
      inputs:
        azureSubscription: '$(azureSubscription)'
        scriptType: bash
        scriptLocation: inlineScript
        inlineScript: |
          set -e
          cd $(tfDir)

          terraform --version || true

          cat > backend.hcl <<EOF
          resource_group_name  = "$(tfBackendRg)"
          storage_account_name = "$(tfBackendSa)"
          container_name       = "$(tfBackendContainer)"
          key                  = "$(tfBackendKey)"
          EOF

          terraform init -backend-config=backend.hcl
          terraform validate
          terraform plan -out $(tfPlanOut)

    - publish: $(System.DefaultWorkingDirectory)/$(tfDir)/$(tfPlanOut)
      artifact: tfplan

    # Manual validation (pause for approval)
    - task: ManualValidation@0
      timeoutInMinutes: 1440
      inputs:
        notifyUsers: ''
        instructions: |
          Review Terraform plan artifact (tfplan).
          Approve to continue to APPLY, or reject to stop.
        onTimeout: 'reject'
```

---

# 5) Terraform apply ‚Äî `pipelines/templates/tf-apply.yml`

```yaml
jobs:
- job: TF_Apply
  displayName: "Terraform: apply"
  dependsOn: TF_Plan
  pool: { vmImage: $(vmImageLinux) }
  steps:
    - download: current
      artifact: tfplan

    - task: AzureCLI@2
      displayName: "Terraform apply"
      inputs:
        azureSubscription: '$(azureSubscription)'
        scriptType: bash
        scriptLocation: inlineScript
        inlineScript: |
          set -e
          cd $(tfDir)
          terraform apply -auto-approve $(Pipeline.Workspace)/tfplan/$(tfPlanOut)
```

> **Pro tip:** For stricter change control, use **Environments** with approvals (Project Settings ‚Üí Pipelines ‚Üí Environments) on this stage.

---

# 6) Deploy to AKS ‚Äî `pipelines/templates/deploy-aks.yml`

```yaml
jobs:
- deployment: Deploy_AKS
  displayName: "AKS: helm upgrade --install"
  environment: 'prod'   # add approvals here if you want
  pool: { vmImage: $(vmImageLinux) }
  strategy:
    runOnce:
      deploy:
        steps:
          - checkout: self

          - task: HelmInstaller@1
            inputs:
              helmVersionToInstall: 'latest'

          - task: HelmDeploy@0
            displayName: "Helm upgrade/install"
            inputs:
              connectionType: 'Kubernetes Service Connection'
              kubernetesServiceConnection: '$(kubeServiceConnection)'
              namespace: '$(aksNamespace)'
              command: 'upgrade'
              chartType: 'FilePath'
              chartPath: '$(helmChartPath)'
              releaseName: 'yourapp'
              valueFile: '$(helmChartPath)/values.yaml'
              arguments: >
                --install
                --set image.repository=$(imageName)
                --set image.tag=$(Build.BuildId)
                --set image.pullPolicy=IfNotPresent
                --wait --timeout 5m
```

---

# 7) Deploy to App Service (blue/green via slot) ‚Äî `pipelines/templates/deploy-appsvc.yml`

```yaml
jobs:
- deployment: Deploy_AppService
  displayName: "App Service: slot deploy + swap"
  environment: 'prod'
  pool: { vmImage: $(vmImageLinux) }
  strategy:
    runOnce:
      deploy:
        steps:
          - ${{ if eq(variables['containerized'], 'true') }}:
            - task: AzureWebAppContainer@1
              displayName: "Deploy container to staging slot"
              inputs:
                azureSubscription: '$(azureSubscription)'
                appName: '$(appServiceName)'
                resourceGroupName: '$(appServiceRG)'
                slotName: '$(appServiceSlot)'
                containers: |
                  $(acrLoginServer)/$(imageName#$(acrLoginServer)/):$(Build.BuildId)
                # If single container, above resolves to gurdianacr.azurecr.io/ai-guardian/webapp:$(Build.BuildId)

          - ${{ if ne(variables['containerized'], 'true') }}:
            - download: current
              artifact: drop
            - task: AzureWebApp@1
              displayName: "Zip deploy to staging slot"
              inputs:
                azureSubscription: '$(azureSubscription)'
                appName: '$(appServiceName)'
                resourceGroupName: '$(appServiceRG)'
                package: '$(Pipeline.Workspace)/drop/drop.zip'
                deployToSlotOrASE: true
                resourceGroupName: '$(appServiceRG)'
                slotName: '$(appServiceSlot)'

          - task: AzureAppServiceManage@0
            displayName: "Swap slots: staging ‚Üí production"
            inputs:
              azureSubscription: '$(azureSubscription)'
              Action: 'Swap Slots'
              WebAppName: '$(appServiceName)'
              ResourceGroupName: '$(appServiceRG)'
              SourceSlot: '$(appServiceSlot)'
              # Target is production by default
```

---

# 8) Deploy to VMs / On-prem ‚Äî `pipelines/templates/deploy-vm.yml`

```yaml
jobs:
- job: Deploy_VM
  displayName: "VMs: copy artifact + restart service"
  pool:
    ${{ if eq(variables['Agent.TargetName'], 'selfhosted') }}:
      name: 'OnPremAgents'     # override when using self-hosted target
    ${{ if ne(variables['Agent.TargetName'], 'selfhosted') }}:
      vmImage: $(vmImageLinux)

  steps:
    - download: current
      artifact: drop

    # Example: Linux VM over SSH
    - task: CopyFilesOverSSH@0
      displayName: "Copy package to VM"
      inputs:
        sshEndpoint: 'ssh-conn'         # service connection to VM
        sourceFolder: '$(Pipeline.Workspace)/drop'
        contents: 'drop.zip'
        targetFolder: '/opt/yourapp'

    - task: SSH@0
      displayName: "Unzip + restart service"
      inputs:
        sshEndpoint: 'ssh-conn'
        script: |
          set -e
          cd /opt/yourapp
          unzip -o drop.zip
          sudo systemctl restart yourapp.service
```

> For **self-hosted** on-prem, create a **private agent** in pool `OnPremAgents` and (optionally) set a variable or separate pipeline run with `parameters.target: selfhosted`.

---

# 9) Post-Deploy checks ‚Äî `pipelines/templates/postdeploy.yml`

```yaml
parameters:
  - name: runZap
    type: boolean
    default: false

jobs:
- job: Smoke_Tests
  displayName: "Smoke tests (Newman)"
  pool: { vmImage: $(vmImageLinux) }
  steps:
    - checkout: self
    - bash: |
        set -e
        docker run --rm -v $(System.DefaultWorkingDirectory)/tests/postman:/etc/newman \
          postman/newman:alpine run /etc/newman/collection.json \
          -e /etc/newman/environment.json \
          --reporters cli,junit \
          --reporter-junit-export $(Build.ArtifactStagingDirectory)/newman.xml
      displayName: "Run Postman collection"

    - task: PublishTestResults@2
      inputs:
        testResultsFormat: 'JUnit'
        testResultsFiles: '$(Build.ArtifactStagingDirectory)/newman.xml'
        failTaskOnFailedTests: true

- ${{ if eq(parameters.runZap, true) }}:
  - job: DAST_ZAP
    displayName: "DAST (OWASP ZAP Baseline)"
    pool: { vmImage: $(vmImageLinux) }
    steps:
      - bash: |
          set -e
          mkdir -p $(Build.ArtifactStagingDirectory)/zap
          docker run --rm -t owasp/zap2docker-stable zap-baseline.py \
            -t $(appUrl) \
            -J $(Build.ArtifactStagingDirectory)/zap/zap.json \
            -r $(Build.ArtifactStagingDirectory)/zap/report.html \
            -m 5
        displayName: "ZAP Baseline scan"
      - task: PublishBuildArtifacts@1
        inputs:
          PathtoPublish: '$(Build.ArtifactStagingDirectory)/zap'
          ArtifactName: 'zap'
          publishLocation: 'Container'
```

---

# 10) Observability ‚Äî `pipelines/templates/observability.yml`

```yaml
parameters:
  - name: target
    type: string
    default: 'appsvc'

jobs:
- job: Mark_Deployment
  displayName: "Push deployment marker (App Insights / Grafana)"
  pool: { vmImage: $(vmImageLinux) }
  steps:
    # Application Insights marker via ingestion API
    - bash: |
        set -e
        if [ -z "$(APPINSIGHTS_IKEY)" ]; then
          echo "APPINSIGHTS_IKEY not set, skipping AI marker."
          exit 0
        fi
        cat > payload.json <<EOF
        [{
          "name":"Microsoft.ApplicationInsights.Event",
          "time":"$(date -u +%Y-%m-%dT%H:%M:%SZ)",
          "iKey":"$(APPINSIGHTS_IKEY)",
          "data":{
            "baseType":"EventData",
            "baseData":{
              "name":"Deployment",
              "properties":{
                "buildId":"$(Build.BuildId)",
                "releaseTarget":"${{ parameters.target }}",
                "pipeline":"$(Build.DefinitionName)",
                "sourceVersion":"$(Build.SourceVersion)"
              }
            }
          }
        }]
        EOF
        curl -sS -X POST https://dc.services.visualstudio.com/v2/track \
          -H "Content-Type: application/json" \
          --data @payload.json || true
      displayName: "App Insights deployment marker"

    # Optional Grafana annotation
    - bash: |
        set -e
        if [ -z "$(GRAFANA_URL)" ] || [ -z "$(GRAFANA_TOKEN)" ]; then
          echo "Grafana vars not set, skipping annotation."
          exit 0
        fi
        cat > ann.json <<EOF
        {
          "time": $(date +%s)000,
          "tags": ["deploy","${{ parameters.target }}"],
          "text": "Deployed build $(Build.BuildId) to ${{ parameters.target }}"
        }
        EOF
        curl -sS -X POST "$(GRAFANA_URL)/api/annotations" \
          -H "Authorization: Bearer $(GRAFANA_TOKEN)" \
          -H "Content-Type: application/json" \
          --data @ann.json || true
      displayName: "Grafana annotation"

# (Optional) Rollback helpers if previous stage failed
- job: Rollback_Hints
  displayName: "Alerts + Rollback guidance"
  condition: failed()
  pool: { vmImage: $(vmImageLinux) }
  steps:
    - bash: |
        echo "Deployment failed. Suggested actions:"
        echo "1) Check Post-Deploy smoke/DAST artifacts, App Service logs (App Insights Live Metrics), AKS pod logs."
        echo "2) Quick rollback:"
        echo "   - App Service: swap slots back: staging -> production (reverse)."
        echo "   - AKS: helm rollback yourapp 1"
        echo "   - VMs: revert package and restart service."
        echo "3) Create an incident with build ID $(Build.BuildId)."
      displayName: "Rollback guidance (human-readable)"
```

---

## How to run this like a pro (step-by-step)

1. **Create variable group `prod-secrets`**

   * `SONAR_PROJECT_KEY`, `SONAR_PROJECT_NAME`, `SONAR_ORG`
   * `APPINSIGHTS_IKEY` (or connection string vars if you adapt)
   * If using Grafana: `GRAFANA_URL`, `GRAFANA_TOKEN`
   * If VM deploy: service connections `ssh-conn` with key.

2. **Service connections**

   * `sc-azure` (Azure subscription)
   * `sc-acr` (ACR registry)
   * `kube-conn` (Kubernetes)
   * (Optional) `ssh-conn` (SSH endpoint to VM)

3. **Infra ready**

   * Put Terraform in `/infra/terraform`. Backend storage (SA/container) must exist (or create once).
   * `terraform plan` waits for **ManualValidation** approval.

4. **Choose target** when queuing the pipeline

   * `target = aks | appsvc | vm | selfhosted`
   * `runZap = true` to enable ZAP baseline
   * `containerized = true/false` depending on your app

5. **Blue/Green (App Service)**

   * Deployed to **staging** slot, then **swap** to production. Instant rollback = swap back.

6. **Post-deploy**

   * Newman smoke tests must target your **live URL**; update `appUrl` and Postman env.

7. **Observability**

   * App Insights & (optional) Grafana get a **deployment marker** tied to Build ID.

---

## Why this rocks in a real company

* **Component-based**: swap deploy targets without rewriting pipelines.
* **Templates + params + variables**: clean, maintainable, re-usable.
* **Security baked in**: Sonar, Trivy, SBOM.
* **Governance**: TF plan approval + Environment gates ready.
* **Ops-friendly**: smoke, optional DAST, markers, rollback hints.

If you paste these files as-is, change the obvious names, and wire up the service connections, you‚Äôll have a **real, production-grade** pipeline your team can ship with. Want me to tailor this precisely to **your current repo and ACR names**, and include a **Helm values.yaml** for AKS too?

So in real time while mentioning variables parameters in YAML file do we need to crate a common re0usable yaml files in real time environments ? so after creating this YAML file we need to pass this parameters values in main yaml ?

This is my doubt just to make things clear and get clarity piece of code blocks with variables and parametrs getting confusion  , need full yaml structure with variables and parameters and how it works to pass ?


You‚Äôve got it right: in real projects we **split** YAML into

1. **Variables (common + per-env)** ‚Üí reusable values
2. **Templates (with parameters)** ‚Üí reusable logic
3. **Main pipeline** ‚Üí imports variables, **passes parameter values** to templates, picks target env

Below is a **clean, minimal, production-style example** that shows exactly how they connect.

---

# üóÇ Folder layout

```
/azure-pipelines.yml                 # MAIN
/pipelines/vars/
  common.yml                         # shared variables
  dev.yml                            # env overrides
  prod.yml
/pipelines/templates/
  ci.yml                             # template with parameters
  deploy.yml                         # template with parameters
```

---

# 1) Variables (shared) ‚Äî `pipelines/vars/common.yml`

```yaml
# variables template (no "parameters" here)
variables:
  # Tooling
  vmImageLinux: 'ubuntu-latest'
  vmImageWindows: 'windows-latest'

  # App
  appName: 'guardianApp'
  imageRepository: 'gurdianacr.azurecr.io/ai-guardian/webapp'

  # Azure service connections / names
  azureSubscription: 'sc-azure'
  acrServiceConnection: 'sc-acr'
  resourceGroup: 'guardian-rg'

  # App Service defaults
  appServiceName: 'your-appservice-name'
  appServiceSlot: 'staging'

  # AKS defaults
  kubeServiceConnection: 'kube-conn'
  aksNamespace: 'prod'

  # URLs for tests
  appUrl: 'https://your-prod-url'
```

## 1b) Variables (per environment) ‚Äî `pipelines/vars/dev.yml`

```yaml
variables:
  environment: 'dev'
  replicas: 1
  enableZap: false
```

`pipelines/vars/prod.yml`

```yaml
variables:
  environment: 'prod'
  replicas: 3
  enableZap: true
```

> Use **Library ‚Üí Variable Groups** for real secrets (tokens, passwords), and reference them in the main file with `- group: prod-secrets`.

---

# 2) Template WITH PARAMETERS (CI) ‚Äî `pipelines/templates/ci.yml`

```yaml
# This template accepts parameters (compile-time)
parameters:
  - name: containerized
    type: boolean
    default: true
  - name: buildConfiguration
    type: string
    default: 'Release'

jobs:
- job: CI
  displayName: "CI: restore ‚Ä¢ build ‚Ä¢ test"
  pool: { vmImage: $(vmImageWindows) }      # <- variable from common.yml
  steps:
    - checkout: self
    - task: NuGetToolInstaller@1
    - task: NuGetCommand@2
      inputs:
        command: 'restore'
        restoreSolution: '**/*.sln'

    - task: VSBuild@1
      inputs:
        solution: '**/*.sln'
        msbuildArgs: '/p:Configuration=${{ parameters.buildConfiguration }}'
        platform: 'Any CPU'
        configuration: '${{ parameters.buildConfiguration }}'

    - task: VSTest@2
      inputs:
        testSelector: 'testAssemblies'
        testAssemblyVer2: |
          **\*test*.dll
          !**\*TestAdapter.dll
          !**\obj\**

    # Optional container build & push
    - ${{ if eq(parameters.containerized, true) }}:
      - task: Docker@2
        displayName: "Docker: build & push"
        inputs:
          command: 'buildAndPush'
          containerRegistry: '$(acrServiceConnection)'
          repository: '$(imageRepository)'
          dockerfile: '$(Build.SourcesDirectory)/Dockerfile'
          tags: |
            $(Build.BuildId)
            latest

    # Publish artifact (useful for App Service/VM deploys)
    - task: ArchiveFiles@2
      inputs:
        rootFolderOrFile: '$(Build.SourcesDirectory)'
        archiveType: 'zip'
        archiveFile: '$(Build.ArtifactStagingDirectory)/drop.zip'
        includeRootFolder: false

    - task: PublishBuildArtifacts@1
      inputs:
        PathtoPublish: '$(Build.ArtifactStagingDirectory)'
        ArtifactName: 'drop'
```

---

# 3) Template WITH PARAMETERS (Deploy) ‚Äî `pipelines/templates/deploy.yml`

This single template can deploy to **aks** or **appsvc** based on a parameter.

```yaml
parameters:
  - name: target
    type: string
    default: 'appsvc'     # values: aks, appsvc
  - name: imageTag
    type: string
    default: 'latest'

jobs:
# ‚îÄ‚îÄ AKS path ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
- ${{ if eq(parameters.target, 'aks') }}:
  - deployment: Deploy_AKS
    displayName: "AKS: helm upgrade --install"
    environment: $(environment)                  # <- from env vars file
    pool: { vmImage: $(vmImageLinux) }
    strategy:
      runOnce:
        deploy:
          steps:
            - task: HelmInstaller@1
              inputs:
                helmVersionToInstall: 'latest'

            - task: HelmDeploy@0
              inputs:
                connectionType: 'Kubernetes Service Connection'
                kubernetesServiceConnection: '$(kubeServiceConnection)'
                namespace: '$(aksNamespace)'
                command: 'upgrade'
                chartType: 'FilePath'
                chartPath: 'charts/yourapp'
                releaseName: 'yourapp'
                arguments: >
                  --install
                  --set image.repository=$(imageRepository)
                  --set image.tag=${{ parameters.imageTag }}
                  --wait --timeout 5m

# ‚îÄ‚îÄ App Service path ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
- ${{ if eq(parameters.target, 'appsvc') }}:
  - deployment: Deploy_AppService
    displayName: "App Service: slot deploy + swap"
    environment: $(environment)
    pool: { vmImage: $(vmImageLinux) }
    strategy:
      runOnce:
        deploy:
          steps:
            # If containerized App Service
            - task: AzureWebAppContainer@1
              displayName: "Deploy container to staging slot"
              inputs:
                azureSubscription: '$(azureSubscription)'
                appName: '$(appServiceName)'
                slotName: '$(appServiceSlot)'
                containers: |
                  $(imageRepository):${{ parameters.imageTag }}

            # (If zip deploy instead, download artifact and use AzureWebApp@1)

            - task: AzureAppServiceManage@0
              displayName: "Swap slots: staging ‚Üí production"
              inputs:
                azureSubscription: '$(azureSubscription)'
                Action: 'Swap Slots'
                WebAppName: '$(appServiceName)'
                SourceSlot: '$(appServiceSlot)'
```

---

# 4) MAIN pipeline ‚Äî `azure-pipelines.yml`

This is where you **import variables** and **pass parameter values** to templates.

```yaml
trigger:
  - main

# Top-level parameters chosen at queue time (compile-time)
parameters:
  - name: environment
    displayName: "Environment"
    type: string
    default: 'dev'
    values: ['dev', 'prod']

  - name: target
    displayName: "Deploy Target"
    type: string
    default: 'appsvc'
    values: ['aks', 'appsvc']

  - name: containerized
    type: boolean
    default: true

  - name: buildConfiguration
    type: string
    default: 'Release'

# Bring in common variables
variables:
- template: pipelines/vars/common.yml

# Bring in env-specific variables based on parameter
- ${{ if eq(parameters.environment, 'dev') }}:
  - template: pipelines/vars/dev.yml
- ${{ if eq(parameters.environment, 'prod') }}:
  - template: pipelines/vars/prod.yml

# Optionally bring in secret variable group
- group: prod-secrets

stages:

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ CI ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
- stage: CI
  displayName: "CI"
  jobs:
  - template: pipelines/templates/ci.yml
    parameters:
      containerized: ${{ parameters.containerized }}
      buildConfiguration: ${{ parameters.buildConfiguration }}

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Deploy ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
- stage: Deploy
  displayName: "Deploy (${{ parameters.target }}) to ${{ parameters.environment }}"
  dependsOn: CI
  condition: succeeded()
  jobs:
  - template: pipelines/templates/deploy.yml
    parameters:
      target: ${{ parameters.target }}
      imageTag: $(Build.BuildId)    # variable evaluated at runtime
```

---

## How it works (no confusion):

* **Parameters** (`${{ }}`)

  * Decided at **compile time** (when the run starts or when templates are expanded).
  * Use them to **control structure/flow** and **pass options into templates**.
  * Example: `target`, `environment`, `containerized`, `buildConfiguration`.

* **Variables** (`$( )`)

  * Resolved at **runtime** (during job execution).
  * Good for values that might come from **variable templates**, **variable groups**, or **system variables** (like `$(Build.BuildId)`).

* **Where to put what**

  * Put **long-lived constants** in **variables templates** (`common.yml`, `prod.yml`).
  * Put **choices per run** in **top-level parameters** of `azure-pipelines.yml`.
  * Put **reusable logic** into **templates** and expose **parameters** there so the main pipeline can control behavior.

---

## Mini mental model

* Main pipeline = **orchestrator**

  * Chooses env & target using **parameters**
  * Imports **variables** (common + per-env)
  * Calls templates **passing parameters**

* Templates = **reusable functions**

  * Receive **parameters**
  * Use **variables** loaded by the main file (e.g., service connection names, URLs)

---

## Common pitfalls (avoid these)

1. Using `$(var)` inside `${{ if ... }}` blocks ‚Üí won‚Äôt work (that‚Äôs compile-time).
2. Forgetting to include variable templates before templates that read them.
3. Tabs in YAML indentation (use spaces).
4. Secrets in YAML ‚Üí put them in **Variable Groups**.
5. Hardcoding environment values across many files ‚Üí keep them in **vars/dev.yml** & **vars/prod.yml**.

---

If you drop these files into your repo and just change a few names (service connections, app names), you‚Äôll see the full flow:

1. Queue run ‚Üí choose **environment=prod** and **target=aks**
2. CI builds & (optionally) pushes image
3. Deploy template sees `target=aks` ‚Üí runs Helm upgrade with image tag `$(Build.BuildId)`
4. All constants (namespace, URLs, service connections) came from **variables templates**

Want me to adapt this exactly to your **`gurdianacr.azurecr.io`**, App Service name, and your **AKS Helm values** so you can run it immediately?

Great question. In real projects with **Dev / QA / Testing / Prod**, you usually do one of these:

1. **Choose ONE environment per run** (ad-hoc deploy)
2. **Promote through ALL environments** in order (Dev ‚Üí QA ‚Üí Testing ‚Üí Prod) with approvals

Below I‚Äôve given you both patterns with clean, copy-pasteable YAML. You‚Äôll see exactly **where variables live**, **how parameters are passed**, and **how env-specific secrets/values are selected**.

---

# üóÇ Folder layout

```
/azure-pipelines.yml                       # MAIN
/pipelines/templates/
  deploy.yml                                # Reusable deploy logic (takes parameters)
/pipelines/vars/
  common.yml                                # Shared variables
  dev.yml
  qa.yml
  testing.yml
  prod.yml
```

---

# 1) Variables (shared + per environment)

`pipelines/vars/common.yml`

```yaml
variables:
  vmImageLinux: 'ubuntu-latest'
  vmImageWindows: 'windows-latest'

  # Reusable names/service connections
  azureSubscription: 'sc-azure'
  acrServiceConnection: 'sc-acr'
  kubeServiceConnection: 'kube-conn'

  # App
  appName: 'guardianApp'
  imageRepository: 'gurdianacr.azurecr.io/ai-guardian/webapp'

  # AKS
  aksNamespace: 'default'
  helmChartPath: 'charts/yourapp'

  # App Service (if used)
  appServiceName: 'your-appservice'
  appServiceSlot: 'staging'
```

`pipelines/vars/dev.yml`

```yaml
variables:
  environment: 'dev'
  replicas: 1
  hostUrl: 'https://dev.example.com'
  variableGroup: 'vg-dev-secrets'   # Library -> Variable Group with secrets for DEV
```

`pipelines/vars/qa.yml`

```yaml
variables:
  environment: 'qa'
  replicas: 1
  hostUrl: 'https://qa.example.com'
  variableGroup: 'vg-qa-secrets'
```

`pipelines/vars/testing.yml`

```yaml
variables:
  environment: 'testing'
  replicas: 2
  hostUrl: 'https://test.example.com'
  variableGroup: 'vg-testing-secrets'
```

`pipelines/vars/prod.yml`

```yaml
variables:
  environment: 'prod'
  replicas: 3
  hostUrl: 'https://prod.example.com'
  variableGroup: 'vg-prod-secrets'
```

> Put sensitive items (DB strings, API keys, etc.) in **Variable Groups** named above and link them in the main pipeline.

---

# 2) Reusable deploy template (parameters in, variables used inside)

`pipelines/templates/deploy.yml`

```yaml
parameters:
  - name: target              # aks | appsvc
    type: string
    default: 'aks'
  - name: imageTag
    type: string
    default: 'latest'

jobs:
- ${{ if eq(parameters.target, 'aks') }}:
  - deployment: Deploy_AKS_${{ variables.environment }}
    displayName: "AKS: ${{ variables.environment }} (helm upgrade --install)"
    environment: ${{ variables.environment }}     # Use ADO Environment for approvals
    pool: { vmImage: $(vmImageLinux) }
    strategy:
      runOnce:
        deploy:
          steps:
            - task: HelmInstaller@1
              inputs:
                helmVersionToInstall: 'latest'

            - task: HelmDeploy@0
              displayName: "helm upgrade --install"
              inputs:
                connectionType: 'Kubernetes Service Connection'
                kubernetesServiceConnection: '$(kubeServiceConnection)'
                namespace: '$(aksNamespace)'
                command: 'upgrade'
                chartType: 'FilePath'
                chartPath: '$(helmChartPath)'
                releaseName: '$(appName)'
                arguments: >
                  --install
                  --set image.repository=$(imageRepository)
                  --set image.tag=${{ parameters.imageTag }}
                  --set replicaCount=$(replicas)
                  --wait --timeout 5m

- ${{ if eq(parameters.target, 'appsvc') }}:
  - deployment: Deploy_AppSvc_${{ variables.environment }}
    displayName: "App Service: ${{ variables.environment }} (slot deploy + swap)"
    environment: ${{ variables.environment }}
    pool: { vmImage: $(vmImageLinux) }
    strategy:
      runOnce:
        deploy:
          steps:
            - task: AzureWebAppContainer@1
              displayName: "Deploy container to staging slot"
              inputs:
                azureSubscription: '$(azureSubscription)'
                appName: '$(appServiceName)'
                slotName: '$(appServiceSlot)'
                containers: |
                  $(imageRepository):${{ parameters.imageTag }}

            - task: AzureAppServiceManage@0
              displayName: "Swap slots: staging ‚Üí production"
              inputs:
                azureSubscription: '$(azureSubscription)'
                Action: 'Swap Slots'
                WebAppName: '$(appServiceName)'
                SourceSlot: '$(appServiceSlot)'
```

---

# PATTERN A ‚Äî Choose ONE environment per run

This is for ‚Äúdeploy to **Dev** only‚Äù or ‚Äúdeploy to **Prod** now‚Äù.

`azure-pipelines.yml`

```yaml
trigger:
  - main

# Choose env & target at queue time
parameters:
  - name: environment
    type: string
    default: 'dev'
    values: ['dev','qa','testing','prod']

  - name: target
    type: string
    default: 'aks'
    values: ['aks','appsvc']

  - name: buildConfiguration
    type: string
    default: 'Release'

variables:
- template: pipelines/vars/common.yml

# Bring in env variables conditionally
- ${{ if eq(parameters.environment, 'dev') }}:
  - template: pipelines/vars/dev.yml
- ${{ if eq(parameters.environment, 'qa') }}:
  - template: pipelines/vars/qa.yml
- ${{ if eq(parameters.environment, 'testing') }}:
  - template: pipelines/vars/testing.yml
- ${{ if eq(parameters.environment, 'prod') }}:
  - template: pipelines/vars/prod.yml

# Link the environment‚Äôs secret variable group
- group: ${{ variables.variableGroup }}

stages:
- stage: CI
  displayName: "CI"
  jobs:
    - job: BuildTest
      pool: { vmImage: $(vmImageWindows) }
      steps:
        - checkout: self
        - task: NuGetToolInstaller@1
        - task: NuGetCommand@2
          inputs:
            command: 'restore'
            restoreSolution: '**/*.sln'
        - task: VSBuild@1
          inputs:
            solution: '**/*.sln'
            msbuildArgs: '/p:Configuration=${{ parameters.buildConfiguration }}'
            platform: 'Any CPU'
            configuration: '${{ parameters.buildConfiguration }}'
        - task: VSTest@2
          inputs:
            testSelector: 'testAssemblies'
            testAssemblyVer2: |
              **\*test*.dll
              !**\*TestAdapter.dll
              !**\obj\**
        - task: Docker@2
          displayName: "Docker: build & push"
          inputs:
            command: 'buildAndPush'
            containerRegistry: '$(acrServiceConnection)'
            repository: '$(imageRepository)'
            dockerfile: '$(Build.SourcesDirectory)/Dockerfile'
            tags: |
              $(Build.BuildId)
              latest

- stage: Deploy
  displayName: "Deploy to ${{ parameters.environment }} (${{ parameters.target }})"
  dependsOn: CI
  jobs:
    - template: pipelines/templates/deploy.yml
      parameters:
        target: ${{ parameters.target }}
        imageTag: $(Build.BuildId)
```

**How it works:**

* You select **environment** and **target** when you run.
* The main pipeline **includes the correct env variables file** and **links that env‚Äôs secret variable group**.
* The **deploy template** gets `target` and the `imageTag` and uses the env variables loaded above.

---

# PATTERN B ‚Äî Promote through ALL environments (Dev ‚Üí QA ‚Üí Testing ‚Üí Prod)

This is common for release pipelines. Each stage uses its **own env vars** & **approval gate**.

`azure-pipelines.yml`

```yaml
trigger:
  - main

parameters:
  - name: target
    type: string
    default: 'aks'
    values: ['aks','appsvc']

variables:
- template: pipelines/vars/common.yml

stages:
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ CI once ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
- stage: CI
  displayName: "CI"
  jobs:
    - job: BuildTestAndPush
      pool: { vmImage: $(vmImageWindows) }
      steps:
        - checkout: self
        - task: NuGetToolInstaller@1
        - task: NuGetCommand@2
          inputs:
            command: 'restore'
            restoreSolution: '**/*.sln'
        - task: VSBuild@1
          inputs:
            solution: '**/*.sln'
            msbuildArgs: '/p:Configuration=Release'
            platform: 'Any CPU'
            configuration: 'Release'
        - task: VSTest@2
          inputs:
            testSelector: 'testAssemblies'
            testAssemblyVer2: |
              **\*test*.dll
              !**\*TestAdapter.dll
              !**\obj\**
        - task: Docker@2
          displayName: "Docker: build & push"
          inputs:
            command: 'buildAndPush'
            containerRegistry: '$(acrServiceConnection)'
            repository: '$(imageRepository)'
            dockerfile: '$(Build.SourcesDirectory)/Dockerfile'
            tags: |
              $(Build.BuildId)

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ DEV ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
- stage: Deploy_Dev
  displayName: "Deploy: Dev"
  dependsOn: CI
  variables:
  - template: pipelines/vars/dev.yml
  - group: ${{ variables.variableGroup }}
  jobs:
  - template: pipelines/templates/deploy.yml
    parameters:
      target: ${{ parameters.target }}
      imageTag: $(Build.BuildId)

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ QA ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
- stage: Deploy_QA
  displayName: "Deploy: QA"
  dependsOn: Deploy_Dev
  variables:
  - template: pipelines/vars/qa.yml
  - group: ${{ variables.variableGroup }}
  jobs:
  - template: pipelines/templates/deploy.yml
    parameters:
      target: ${{ parameters.target }}
      imageTag: $(Build.BuildId)

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ TESTING ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
- stage: Deploy_Testing
  displayName: "Deploy: Testing"
  dependsOn: Deploy_QA
  variables:
  - template: pipelines/vars/testing.yml
  - group: ${{ variables.variableGroup }}
  jobs:
  - template: pipelines/templates/deploy.yml
    parameters:
      target: ${{ parameters.target }}
      imageTag: $(Build.BuildId)

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ PROD ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
- stage: Deploy_Prod
  displayName: "Deploy: Prod"
  dependsOn: Deploy_Testing
  variables:
  - template: pipelines/vars/prod.yml
  - group: ${{ variables.variableGroup }}
  jobs:
  - template: pipelines/templates/deploy.yml
    parameters:
      target: ${{ parameters.target }}
      imageTag: $(Build.BuildId)
```

**How promotion works:**

* **Each stage** loads its **own env vars file** and **secret variable group**.
* Use **Azure DevOps Environments** named `dev`, `qa`, `testing`, `prod` and attach **approvals** to them (Project Settings ‚Üí Pipelines ‚Üí Environments).
* The same **deploy template** runs 4 times with different variables.

---

## Quick mental model (no confusion):

* **Variables** `$(var)` ‚Üí values from `common.yml` + `env.yml` + variable groups (resolved at runtime in jobs)
* **Parameters** `${{ parameters.x }}` ‚Üí choices passed from main ‚Üí templates (resolved at compile time; control flow/behavior)
* **Template** ‚Üí reusable logic; reads variables and accepts parameters

---

Want me to plug in your **actual names** (your ACR `gurdianacr.azurecr.io`, App Service name, Helm values) so this runs in your repo with minimal edits?


Short answer: **No.** In real-world teams you **do not** deploy to Dev, QA, Testing, and Prod at the same time.
You **build once** and **promote the same artifact** through environments **sequentially** with quality gates.

## Why not all at once?

* **Risk control:** a bug hits Dev first, not Prod.
* **Signal quality:** each env validates a different thing (Dev = integration, QA = functionality, Testing/UAT = business sign-off, Prod = customers).
* **Compliance & audit:** approvals and change tickets are tied to promotions.
* **Config drift detection:** issues surface earlier when config differs across envs.
* **Rollback simplicity:** easier to stop promotion than undo a 4-env blast.

## When is ‚Äúmultiple at once‚Äù okay?

* **Lower envs only** (e.g., Dev + QA in parallel) to speed feedback‚Äî**never include Prod**.
* **Emergency hotfixes** can skip some gates, but still **separate Prod** with an approval/check.
* **Feature flags**: you can ship the code everywhere ‚Äúdark‚Äù and enable the feature gradually‚Äî**deployment ‚â† release**.

---

## Recommended pattern (production-grade)

1. **CI**: produce an **immutable artifact** (image/tag or zip).
2. **Promotion**: Dev ‚Üí QA ‚Üí Testing ‚Üí Prod with **automated checks + approvals**.
3. **Env-specific config**: via variable templates/groups (no code changes).
4. **Gates**: quality gate (Sonar), smoke tests (Newman), optional DAST, manual/jira approval for Prod.
5. **Safe release**: blue/green or canary in Prod; instant rollback.

### Minimal Azure DevOps YAML (sequential promotion)

```yaml
trigger:
  - main

variables:
- template: pipelines/vars/common.yml

stages:
- stage: CI
  jobs:
  - job: Build
    steps:
      - checkout: self
      - task: Docker@2
        inputs:
          command: buildAndPush
          containerRegistry: '$(acrServiceConnection)'
          repository: '$(imageRepository)'
          dockerfile: '$(Build.SourcesDirectory)/Dockerfile'
          tags: |
            $(Build.BuildId)

# ---- Dev ----
- stage: Deploy_Dev
  dependsOn: CI
  variables:
  - template: pipelines/vars/dev.yml
  - group: ${{ variables.variableGroup }}
  jobs:
  - template: pipelines/templates/deploy.yml
    parameters:
      target: aks
      imageTag: $(Build.BuildId)

# ---- QA (auto after Dev success) ----
- stage: Deploy_QA
  dependsOn: Deploy_Dev
  variables:
  - template: pipelines/vars/qa.yml
  - group: ${{ variables.variableGroup }}
  # add an Environment named 'qa' in ADO and attach approvals/checks there
  jobs:
  - template: pipelines/templates/deploy.yml
    parameters:
      target: aks
      imageTag: $(Build.BuildId)

# ---- Testing/UAT (manual approval gate) ----
- stage: Deploy_Testing
  dependsOn: Deploy_QA
  variables:
  - template: pipelines/vars/testing.yml
  - group: ${{ variables.variableGroup }}
  jobs:
  - template: pipelines/templates/deploy.yml
    parameters:
      target: aks
      imageTag: $(Build.BuildId)

# ---- Prod (strict gates + blue/green/canary) ----
- stage: Deploy_Prod
  dependsOn: Deploy_Testing
  variables:
  - template: pipelines/vars/prod.yml
  - group: ${{ variables.variableGroup }}
  # Use an ADO Environment named 'prod' with required approvers and checks
  jobs:
  - template: pipelines/templates/deploy.yml
    parameters:
      target: aks
      imageTag: $(Build.BuildId)
```

> Put **approvals/checks on Environments** (Project Settings ‚Üí Pipelines ‚Üí Environments: dev/qa/testing/prod).
> Add automatic **smoke tests** as part of each stage; add **ZAP/DAST** and **ManualValidation** before Prod if needed.

---

## TL;DR

* **Best practice:** sequential promotion with gates; **never** blast all four at once.
* **Speed AND safety:** parallelize **lower envs**, keep **Prod** separate with strict checks.
* **Ship dark, release later** with feature flags for safer Prod rollouts.

If you want, I‚Äôll plug this into your existing repo layout and wire it to **AKS** and **App Service** with real approvals and smoke tests.


Got it üëç ‚Äî let‚Äôs write a **PowerShell script for Azure Function App rollback**.
In real-time orgs, rollback usually means:

1. **Swap back to previous deployment slot** (Blue/Green style), OR
2. **Re-deploy a previous version** (from artifact storage).

üëâ The **recommended way in Azure Function Apps** = **Deployment Slots**.
You deploy new code to `staging`, swap with `production`, and rollback by swapping back.

---

# üß± PowerShell Script ‚Äì Rollback Azure Function App (Slot Swap)

```powershell
param(
    [string]$resourceGroup = "guardian-rg",
    [string]$functionAppName = "my-function-app",
    [string]$sourceSlot = "production",
    [string]$targetSlot = "staging"
)

# Login to Azure
Write-Output "Logging into Azure..."
Connect-AzAccount -UseDeviceAuthentication

# Swap slots (rollback)
Write-Output "Rolling back Function App '$functionAppName' in RG '$resourceGroup'"
Write-Output "Swapping slot '$targetSlot' back into '$sourceSlot'..."

Swap-AzWebAppSlot `
    -ResourceGroupName $resourceGroup `
    -Name $functionAppName `
    -SourceSlotName $targetSlot `
    -DestinationSlotName $sourceSlot

Write-Output "Rollback completed ‚úÖ"
```

### Usage

```powershell
.\rollback-funcapp.ps1 -resourceGroup "guardian-rg" -functionAppName "guardian-func" -sourceSlot "production" -targetSlot "staging"
```

---

# üß± Alternate Approach ‚Äì Rollback to Previous Artifact

If your org doesn‚Äôt use slots but deploys ZIPs from artifacts:

```powershell
param(
    [string]$resourceGroup = "guardian-rg",
    [string]$functionAppName = "my-function-app",
    [string]$packageUri = "https://myblob.blob.core.windows.net/artifacts/functionapp-v1.zip" # previous good version
)

# Login
Connect-AzAccount -UseDeviceAuthentication

# Redeploy previous package
Publish-AzWebApp `
    -ResourceGroupName $resourceGroup `
    -Name $functionAppName `
    -ArchivePath $packageUri

Write-Output "Rollback deployment completed ‚úÖ"
```

üëâ Here, you maintain **versioned artifacts** (v1.zip, v2.zip) in Azure Storage/Artifacts. Rollback = redeploy v1.

---

# üß± Real Org Best Practices

* Always enable **Deployment Slots** for Function Apps (`staging` + `production`).
* Deploy to **staging**, validate, then **swap** to prod.
* Rollback = simply swap back ‚Üí instant, zero downtime.
* Keep **artifacts versioned** (Build ID in filename) ‚Üí reproducible rollbacks.
* Log rollbacks in **Azure Monitor / App Insights** for audit.

---

‚ö° This script can be plugged into **Azure DevOps pipeline rollback stage**.

üëâ Do you want me to also write you a **YAML pipeline rollback job** (that calls this PowerShell script automatically) so rollback is just one click?


Perfect üíØ ‚Äî you‚Äôve hit **two super practical topics** that every **real-time Azure DevOps / Cloud Engineer** must know:

1. **How to add customized tags to multiple Azure resources**
2. **What Azure Advisor is and how it helps in companies**

Let‚Äôs deep dive üëá

---

# üß± 1. Customized Tags in Azure (Real-Time Practice)

üëâ **Tags = key-value pairs** attached to resources for:

* Cost management (chargeback/showback to teams)
* Ownership (who owns the resource)
* Environment (Dev, QA, Prod)
* Compliance / Security tracking

üìå Real-time orgs **MANDATE tags** (via policies) like:

* `Owner = raghu`
* `Environment = Prod`
* `CostCenter = FinOps`
* `Application = guardianApp`

---

## üîπ A) Add Tags via **Azure Portal** (manual way)

1. Go to **Resource** ‚Üí **Tags** blade.
2. Add `Key = Value` pairs.
3. Click **Apply**.
   üëâ Works but not scalable for 100+ resources.

---

## üîπ B) Add Tags to Multiple Resources (PowerShell)

```powershell
# Login
Connect-AzAccount

# Define tag
$tags = @{"Owner"="Raghu"; "Environment"="Prod"; "CostCenter"="FinOps"}

# Get all resources in a Resource Group
$resources = Get-AzResource -ResourceGroupName "guardian-rg"

# Loop and apply tags
foreach ($res in $resources) {
    Set-AzResource -ResourceId $res.ResourceId -Tag $tags -Force
    Write-Output "Tagged $($res.Name)"
}
```

‚úÖ This applies same tags to **all resources in RG** `guardian-rg`.

---

## üîπ C) Add Tags using **Azure CLI (Bash)**

```bash
# Login
az login

# Add tags to Resource Group (applies to RG object only)
az group update --name guardian-rg --set tags.Owner=Raghu tags.Environment=Prod

# Add tags to specific resource
az resource tag \
  --resource-group guardian-rg \
  --name guardian-aks \
  --resource-type "Microsoft.ContainerService/managedClusters" \
  --tags Owner=Raghu Environment=Prod CostCenter=FinOps
```

---

## üîπ D) Enterprise Way ‚Äî **Azure Policy**

* Create policy: **‚ÄúRequire tags on resources‚Äù**.
* Example: Must have `Environment` and `CostCenter`.
* If someone creates a resource without tags ‚Üí policy denies or auto-adds defaults.

üëâ This is how **companies enforce tagging** for cost governance.

---

# üß± 2. What is Azure Advisor?

üëâ **Azure Advisor = Cloud consultant built into Azure.**
It gives **personalized recommendations** to optimize:

* **Cost**

  * E.g., ‚ÄúYour VM is underutilized, resize from D4 to D2 ‚Üí save ‚Çπ10,000/month.‚Äù
* **Performance**

  * E.g., ‚ÄúEnable CDN for storage account to improve latency.‚Äù
* **Reliability (HA/DR)**

  * E.g., ‚ÄúYour VM isn‚Äôt in an Availability Set/Zone ‚Üí risk of downtime.‚Äù
* **Security**

  * E.g., ‚ÄúEnable Defender for Cloud, enable disk encryption.‚Äù
* **Operational Excellence**

  * E.g., ‚ÄúYour AKS cluster control plane version is out of date ‚Üí upgrade.‚Äù

üìå In real companies:

* **Cloud FinOps team** uses Advisor for cost-saving.
* **DevOps team** uses Advisor alerts for AKS upgrades & VM right-sizing.
* **Security team** integrates Advisor with Defender for Cloud.

---

# üß† Techie Player Formula (easy to explain in interviews / projects)

* **Tags** = metadata for resources
  üëâ ‚ÄúWe use tags like `Owner`, `CostCenter`, `Environment` across all resources enforced by Azure Policy for cost & compliance.‚Äù

* **Azure Advisor** = smart consultant
  üëâ ‚ÄúAdvisor continuously analyzes our resources and gives us cost, performance, reliability, and security recommendations. We integrate it into our FinOps reviews and Ops dashboards to improve efficiency.‚Äù

---

‚ö° If you show in your company/project that you:

1. Enforce **tagging standards with Policy + automation (PowerShell/CLI)**
2. Use **Azure Advisor recommendations in cost/security governance**

üëâ You‚Äôll look like a **real Cloud Rockstar** ü§ò

---

Would you like me to also give you a **ready-to-use PowerShell + YAML pipeline snippet** that auto-tags all new resources during deployment (so you never miss tags in CI/CD)?

An error occurred while loading the YAML build pipeline. Variable group was not found or is not authorized for use. For authorization details, refer to https://aka.ms/yamlauthz.
This error occurs when the variable group is not properly authorized for use by the pipeline. While the pipeline definition might look correct, Azure DevOps requires explicit permission for a pipeline to access a variable group, especially when it contains secrets. 
Here is a step-by-step guide to resolve the issue.
Method 1: Authorize the failed build
The quickest way to fix this is to authorize the pipeline directly from the failed build run.
Navigate to your failed pipeline run in Azure DevOps.
In the job summary, you will see a message indicating a resource authorization error.
Click the Authorize resources button next to the error. This will explicitly grant your pipeline access to the specific variable group it is trying to use.
After authorizing, re-run the pipeline. 
Method 2: Grant permission via the Library
You can also grant permissions to the variable group directly from the pipeline library, which gives you more control.
In your Azure DevOps project, navigate to Pipelines > Library.
Find and select the variable group that was not authorized (e.g., tf-remote-backend or kv-secrets).
Click the Pipeline permissions tab.
To authorize a specific pipeline, click the + button and select your pipeline from the list.
Alternatively, you can give access to all pipelines by clicking the More actions icon (three dots) and selecting Open access. This is convenient for non-secret variables but not recommended for groups containing sensitive information. 
Method 3: Check variable group existence and name
Another possibility is that the variable group simply doesn't exist or its name is misspelled in your YAML file.
Go to Pipelines > Library in your project and verify that the variable groups named tf-remote-backend and kv-secrets exist.
Ensure that the variable group names in your YAML file exactly match the names in the Azure DevOps Library. 
For the provided YAML
Based on your pipeline, you need to ensure that both tf-remote-backend and kv-secrets are authorized. 
yaml
variables:
  - group: tf-remote-backend
  - group: kv-secrets
Use code with caution.

Apply Method 1 or 2 for each of these variable groups to resolve the authorization error.
AI responses may include mistakes. Learn more




undefined
undefined
11 sites
Manage variable groups - Azure Pipelines - Microsoft Learn
Aug 28, 2024 ‚Äî In this article. ... This article explains how to create and use variable groups in Azure Pipelines. Variable groups store values and secrets that you can pass ...
favicon
Microsoft Learn

Cannot authorize variable group in Azure Pipelines
Oct 10, 2019 ‚Äî If you're having trouble authorizing a variable group in Azure Pipelines, you can try these steps: * **Check the YAML file** Make sure the variable group is imp...
favicon
Stack Overflow

Resources in YAML pipelines - Azure Pipelines | Microsoft Learn
Aug 5, 2025 ‚Äî Resource authorization. Resources must be authorized to be used in pipelines. Resource owners control the users and pipelines that can access their resources. T...
favicon
Microsoft Learn

Common changes in DevOps deployment pipelines during sprint releases
When working with Terraform pipelines, YAML deployment pipelines, and Kubernetes pipelines in a DevOps environment, several changes are typically made during each sprint release to facilitate the deployment of new features, bug fixes, and improvements. 
Here are some common changes:
1. Updating application code
New Features & Fixes: The most frequent change involves integrating new or updated application code from the current sprint. This includes new functionalities, bug fixes, and performance improvements that are ready for deployment.
Version Bumps: Updating the application's version number in relevant configuration files (e.g., package.json, pom.xml, Dockerfile) and within the pipeline definition itself to reflect the new release. 
2. Infrastructure as code (terraform) updates
Resource Modifications: Terraform configurations might be updated to support new features or changes, such as adding new database instances, modifying network configurations, or updating resource sizes.
Variable Updates: Changes in environment variables, sensitive data, or other parameters may require updating Terraform variable files or using secrets management solutions. 
3. Kubernetes deployment manifests (yaml)
Image Updates: Updating the Docker image tags in the Kubernetes deployment YAML files to deploy the latest version of the application container.
Resource Adjustments: Adjusting resource limits (CPU, memory), replica counts, or other Kubernetes-specific settings based on the requirements of the current release.
Configuration Changes: Updating ConfigMaps or Secrets to reflect new application configurations or sensitive data required by the updated application.
Service & Ingress Updates: Changes to Kubernetes services or ingress rules if new endpoints or routing configurations are introduced. 
4. Pipeline configurations (yaml)
Branch Filtering: Configuring triggers to deploy specifically from branches relevant to the sprint (e.g., features/* for feature branches or the main branch for releases).
Stage Definitions: Modifying pipeline stages (e.g., adding a new environment for testing, adjusting the order of deployment stages) or tasks to accommodate changes in the deployment process.
Approvals & Gates: Updating approval mechanisms or adding new gates to ensure quality and compliance before deploying to production environments.
Task Updates: Occasionally, updating the version of tasks used within the pipeline (e.g., if a new major version of a task is available).
Variable Groups: Updating or modifying variable groups to ensure that the correct environment-specific variables are used during deployment. 
5. Environment configuration
Database Schema Changes: Migrations or updates to the database schema may be necessary to support the new features.
External Service Integrations: Updating configurations for integrations with external services or APIs if the application's dependencies change. 
Important considerations
Automation: Most of these changes should be automated and integrated into the CI/CD pipeline to minimize manual errors and ensure consistency.
Testing: Thorough testing, including unit, integration, and end-to-end tests, should be conducted after each release to ensure the stability and functionality of the deployed application.
Rollback Strategy: It is crucial to have a well-defined rollback strategy in case any issues arise during or after deployment, says Microsoft¬†Community.

Important considerations
Automation: Most of these changes should be automated and integrated into the CI/CD pipeline to minimize manual errors and ensure consistency.
Testing: Thorough testing, including unit, integration, and end-to-end tests, should be conducted after each release to ensure the stability and functionality of the deployed application.
Rollback Strategy: It is crucial to have a well-defined rollback strategy in case any issues arise during or after deployment, says Microsoft Community. 

Automated rollbacks
How it works: An automated process, often built into the CI/CD pipeline, detects deployment failures based on health checks, performance metrics, or other predefined criteria. It then automatically triggers a rollback to the last known stable version, without manual intervention.
Pros: Offers the fastest possible recovery time by removing human involvement.
Cons: Requires careful setup and a robust monitoring system to define failure conditions accurately. 
A note on database rollbacks
Databases require special attention during rollbacks due to the stateful nature of data. A simple redeployment of code will not fix a problematic database migration. Strategies include: 
Backwards-compatible migrations: Design new schema changes to work with both the old and new code.
Backup and restore: Take a backup of the database before a major change so it can be restored if needed. This may cause data loss for any changes that occurred after the backup.
Fix forward: Rather than reverting the database, apply another script to fix the initial problematic change. 

Increase the database tier to get more CPU and memory.
Break down large transactions into smaller, more manageable ones.
Connectivity issues:
Diagnose: Check the SQL Database's server logs for connection errors and timeouts.
Resolve: Ensure network rules and firewalls are correctly configured to allow traffic from your application to the database. 
Azure Storage Accounts
Troubleshooting involves addressing access, performance, or connectivity problems with blobs, files, queues, or tables.
Access Denied errors:
Diagnose: Verify the client's credentials and permissions. Check for firewall rules on the storage account that might block the client's IP address.
Resolve: If using shared access signatures (SAS), ensure the token is not expired and has the correct permissions. For identity-based access, confirm the user or service principal has the appropriate Azure role-based access control (RBAC) role.
High latency:
Diagnose: Monitor SuccessE2ELatency and SuccessServerLatency metrics in Azure Monitor. A large difference between them indicates network or client-side issues.
Resolve: For client-side issues, investigate client-side logging and network performance. For server-side latency, consider poor data organization in tables or parallel uploads to a single blob. 
Azure Monitor and Application Insights
These services provide the telemetry essential for real-time troubleshooting.
Missing telemetry in Application Insights:
Diagnose: Check for adaptive sampling, which is enabled by default and might discard a fraction of telemetry. Also, check if your pricing plan's data rate limit is being exceeded.
Resolve: Adjust the sampling rate or upgrade the pricing plan if necessary. You can also use Fiddler to inspect if telemetry is being successfully sent to the back end.
No logs appearing in Log Analytics:
Diagnose: Confirm that diagnostic settings are correctly configured for your resources to send logs to the Log Analytics workspace.
Resolve: Verify network connectivity from the resource to the workspace. Ensure that necessary permissions are in place and that any potential ingestion delays are considered. 
Best practices for real-time troubleshooting
Use a centralized monitoring solution: Configure Azure Monitor and Log Analytics to collect logs and metrics from all services in a central workspace.
Set up comprehensive alerting: Use Azure Monitor alerts with action groups to get proactive notifications via email, SMS, or webhooks when critical metrics exceed predefined thresholds.
Leverage Azure Resource Health: Use the Resource Health dashboard to quickly check the health status of individual resources. It provides insights into platform-level events that could affect your services.
Utilize diagnostic tools: Become familiar with the dedicated troubleshooting features for each service, such as AKS Diagnose and Solve Problems and Network Watcher's Connection Troubleshoot.
Implement automated runbooks: For common issues, create automated runbooks using Azure Automation to perform diagnostic tasks or remediation steps automatically. 

with the new ones, ensuring continuous availability. Zeet.co
Configuration Updates: Updates to ConfigMaps and Secrets are applied to the cluster, ensuring the application receives the correct configuration for the new release. IBM
Helm Chart Updates: If Helm is used, the Helm charts defining the Kubernetes deployments are updated to reflect the new application version and configuration changes. According to IBM 
By managing these changes iteratively within each sprint, teams ensure that the application, its underlying infrastructure, and the deployment process evolve together in a synchronized and efficient manner.

C. Automation (general)
Infrastructure as Code (IaC): Use Terraform to define and manage all Azure resources (AKS cluster, databases, storage, etc.). cloudiseasy.com This ensures infrastructure is version-controlled and reproducible.
CI/CD Pipeline: Define your build, test, and deployment stages as YAML code in Azure Pipelines. Microsoft Community
Continuous Deployment Triggers: Configure your pipeline to automatically trigger a new release when code changes are pushed to specific branches (e.g., main). Microsoft Community 
D. Testing
Unit & Integration Tests: Integrate these tests into your CI stage. Ensure the pipeline executes the test suite after building the application and before pushing the image to ACR.
End-to-End Tests: Include automated end-to-end tests after deploying the application to staging or test environments. These tests validate the full functionality, including database interactions and external service calls. According to Talent500
YAML Test Stages Example:
yaml
- stage: Build
  jobs:
  - job: BuildAndTest
    steps:
    - task: DotNetCoreCLI@2
      displayName: 'Build project'
      inputs:
        command: 'build'
    - task: DotNetCoreCLI@2
      displayName: 'Run Unit and Integration Tests'
      inputs:
        command: 'test'
        projects: '/*.UnitTests.csproj'
        arguments: '--configuration $(BuildConfiguration)'

- stage: DeployToDev
  jobs:
  - deployment: DeployToDev
    environment: 'Dev'
    strategy:
      runOnce:
        deploy:
          steps:
          - task: KubernetesManifest@1
            displayName: 'Deploy to Dev AKS'
            # ... deployment details ...
          - task: CmdLine@2
            displayName: 'Run E2E Tests on Dev Environment'
            inputs:
              script: 'npm run e2e-tests --baseUrl=$(WebAppNameDev)'
Use code with caution.

 
E. Rollback strategy
Kubernetes Rolling Updates: Kubernetes handles rollbacks automatically when a deployment fails. The deployment controller keeps track of previous deployment revisions, allowing you to easily roll back to a stable version using kubectl rollout undo. Microsoft Community
Database Rollbacks:
Backward Compatibility: Design database schema changes to be backward compatible where possible. One2N notes
Migration Tool Features: Utilize migration tools that support rollback scripts or automatically manage the database history. Talent500
Fix-Forward Approach: In complex cases, a "fix-forward" strategy (deploying a new change to correct the issue) may be preferred over a full rollback. 
By following these steps, you can create a robust and automated DevOps pipeline that manages your application code, infrastructure, database, and external service integrations effectively, while ensuring stability and a clear rollback strategy for each sprint release.

No, in most real-time organizations, you typically do not create a new Kubernetes cluster and database for every sprint release. This would be highly inefficient and complex. Instead, best practices in DevOps and cloud management lean towards managing and updating existing infrastructure. 
Here's why and what the common approach involves:
1. Kubernetes Clusters
Existing Production Clusters: Organizations generally maintain one or more production-ready Kubernetes clusters that are continuously updated and managed. These clusters are designed for high availability, scalability, and security, says Kubernetes not for frequent replacement.
Deployment Rollouts: With each sprint release, the new version of your application (container images and Kubernetes manifest changes) is deployed to the existing cluster using a rollout strategy like a rolling update, blue-green deployment, or canary release. Kubernetes handles the update, gradually replacing the old application pods with the new ones without requiring a new cluster.
Infrastructure Updates: Underlying cluster components (e.g., Kubernetes versions, node images, network plugins) are updated and maintained over time, not replaced entirely with every sprint.
Reasons for New Clusters: While not for every sprint, new clusters might be provisioned for:
New environments: Separate clusters for development, testing, staging, and production.
Isolation requirements: For highly sensitive data or critical applications needing physical separation.
Geographical or regulatory reasons: If data must reside in different regions.
Large-scale architectural shifts: A new product line or a complete redesign might warrant a dedicated cluster. 
2. Databases
Persistent Data: Databases are typically stateful, meaning they store critical application data. Creating a new database for every sprint would imply migrating all existing data, which is time-consuming, complex, and prone to error.
Schema Changes: Instead of replacing the database, the most common approach is to apply database schema changes (migrations) using tools like Liquibase or Flyway. These tools manage incremental updates to the existing database schema, ensuring that the database remains compatible with the new application version.
Managed Database Services: Many organizations use managed database services (like Azure SQL Database, Azure Cosmos DB) where the underlying infrastructure and maintenance are handled by Azure, notes zesty.co further reducing the need for frequent database creation.
Database on Kubernetes: While technically possible to run databases on Kubernetes using StatefulSets and Operators, it adds complexity compared to managed services. Even in this scenario, the database instances are updated or scaled, not replaced entirely for every sprint.
Reasons for New Databases: New database instances might be created for:
New environments: Separate database instances for different environments (dev, test, prod) to maintain isolation and data integrity.
Sharding or scaling out: Adding new database shards or replicas to handle increased load or data volume.
New application components: A new microservice might require its own dedicated database instance. 
In summary, the core principle is continuous deployment to existing environments rather than recreating entire environments for each sprint. According to MuleSoft Automation focuses on updating existing infrastructure (via IaC), deploying new application versions, and applying database migrations, all within a robust¬†CI/CD¬†pipeline.

standard load balancer.
Web Application Firewall (WAF): Provides protection against common web vulnerabilities and exploits.
SSL Offloading: Handles SSL/TLS encryption, freeing up the backend VMs to focus on processing application requests.
URL-based Routing: Can route traffic based on the URL path, allowing for more complex traffic management. 
4. Optimize the application for scaling
While an IaaS approach is used for legacy applications, some optimizations can be made.
Session Management: Ensure the application's session management is stateless or uses a shared, external state store (e.g., Azure Cache for Redis). This prevents issues where a user is routed to a different VM and loses their session.
Centralized Logging and Monitoring: All VMs should send logs and metrics to a central location like Azure Monitor. This provides a single pane of glass for monitoring and troubleshooting issues across all instances. 
Final architectural pattern
The complete, production-grade architecture would look like this:
User Request: Traffic comes in from the internet to a single public IP address.
Load Distribution: An Azure Application Gateway or Azure Load Balancer distributes the traffic.
Dynamic Scaling: The traffic is routed to an Azure Virtual Machine Scale Set, which contains multiple identical VMs.
Application Hosting: Each VM in the scale set runs the legacy application.
Data Storage: A separate, managed database service (e.g., Azure SQL Database) stores the application's data. 
The self-hosted agent in this setup is used only for deploying the application to the VMSS. The end-users never interact with the agent VM itself. This architecture addresses the user load concerns by distributing traffic, providing high availability, and enabling automatic¬†scaling.

focus on processing application requests.
URL-based Routing: Can route traffic based on the URL path, allowing for more complex traffic management. 
4. Optimize the application for scaling
While an IaaS approach is used for legacy applications, some optimizations can be made.
Session Management: Ensure the application's session management is stateless or uses a shared, external state store (e.g., Azure Cache for Redis). This prevents issues where a user is routed to a different VM and loses their session.
Centralized Logging and Monitoring: All VMs should send logs and metrics to a central location like Azure Monitor. This provides a single pane of glass for monitoring and troubleshooting issues across all instances. 
Final architectural pattern
The complete, production-grade architecture would look like this:
User Request: Traffic comes in from the internet to a single public IP address.
Load Distribution: An Azure Application Gateway or Azure Load Balancer distributes the traffic.
Dynamic Scaling: The traffic is routed to an Azure Virtual Machine Scale Set, which contains multiple identical VMs.
Application Hosting: Each VM in the scale set runs the legacy application.
Data Storage: A separate, managed database service (e.g., Azure SQL Database) stores the application's data. 
The self-hosted agent in this setup is used only for deploying the application to the VMSS. The end-users never interact with the agent VM itself. This architecture addresses the user load concerns by distributing traffic, providing high availability, and enabling automatic¬†scaling.

That's an excellent and crucial question for production-grade environments. Simply deploying a legacy application to a VM using a self-hosted agent is only one part of the process. Exposing that single VM directly to end-users would absolutely cause performance issues and become a bottleneck, especially under high user load. 
The self-hosted agent's job is to deploy the application, not to serve user requests. The deployed application then needs a proper architectural setup to handle user traffic.
Here is a breakdown of the problem and the best practices for handling it in real-world organizations.
Why a single VM is a bottleneck
Performance: A single VM has a finite amount of CPU, RAM, and network bandwidth. A sudden surge in user traffic could easily exhaust these resources, causing the application to slow down or become unresponsive.
Single Point of Failure: If the VM goes down for any reason (hardware failure, OS crash, etc.), the application becomes unavailable to all users, resulting in downtime.
Maintenance: Any maintenance on that single VM, such as applying OS patches or rebooting, would require taking the application offline, leading to planned downtime.
Scalability: A single VM cannot scale to meet increasing user demand. You are limited to the resources of that one machine. 
How real organizations handle user load on VMs
To make a legacy application deployed to VMs production-grade and scalable, organizations use a combination of Azure services. 
1. Implement a load balancer
The first and most critical step is to place a load balancer in front of multiple application VMs.
Azure Load Balancer: Distributes incoming network traffic across multiple identical VMs. This prevents any single VM from being overloaded.
Health Probes: The load balancer continuously monitors the health of each VM. If a VM fails, the load balancer automatically stops sending traffic to it, ensuring high availability.
Single Entry Point: A single public IP address is assigned to the load balancer, providing a stable entry point for users even if the backend VMs change. 
2. Create a Virtual Machine Scale Set (VMSS)
Instead of manually creating and managing multiple identical VMs, a VMSS automates this process.
Autoscaling: A VMSS can automatically increase or decrease the number of VM instances based on predefined rules. You can configure rules to scale out (add more VMs) when CPU usage is high and scale in (remove VMs) when it is low.
High Availability: By default, VMSS spreads instances across Azure's fault and update domains, maximizing uptime.
Integration with Load Balancer: A VMSS is natively integrated with an Azure Load Balancer to distribute traffic across all instances.
Application Deployment: Your Azure DevOps pipeline, using the self-hosted agent, can be configured to deploy the application to the VMSS, ensuring that new VMs created during a scale-out event automatically get the latest version of the application. 

3. Use an Application Gateway for advanced features
For web applications, an Application Gateway provides more advanced features than a standard load balancer.
Web Application Firewall (WAF): Provides protection against common web vulnerabilities and exploits.
SSL Offloading: Handles SSL/TLS encryption, freeing up the backend VMs to focus on processing application requests.
URL-based Routing: Can route traffic based on the URL path, allowing for more complex traffic management. 
4. Optimize the application for scaling
While an IaaS approach is used for legacy applications, some optimizations can be made.
Session Management: Ensure the application's session management is stateless or uses a shared, external state store (e.g., Azure Cache for Redis). This prevents issues where a user is routed to a different VM and loses their session.
Centralized Logging and Monitoring: All VMs should send logs and metrics to a central location like Azure Monitor. This provides a single pane of glass for monitoring and troubleshooting issues across all instances. 
Final architectural pattern
The complete, production-grade architecture would look like this:
User Request: Traffic comes in from the internet to a single public IP address.
Load Distribution: An Azure Application Gateway or Azure Load Balancer distributes the traffic.
Dynamic Scaling: The traffic is routed to an Azure Virtual Machine Scale Set, which contains multiple identical VMs.
Application Hosting: Each VM in the scale set runs the legacy application.
Data Storage: A separate, managed database service (e.g., Azure SQL Database) stores the application's data. 
The self-hosted agent in this setup is used only for deploying the application to the VMSS. The end-users never interact with the agent VM itself. This architecture addresses the user load concerns by distributing traffic, providing high availability, and enabling automatic¬†scaling.


stage: Build
  jobs:
  - job: BuildAndTest
    steps:
    - task: DotNetCoreCLI@2
      displayName: 'Build project'
      inputs:
        command: 'build'
    - task: DotNetCoreCLI@2
      displayName: 'Run Unit and Integration Tests'
      inputs:
        command: 'test'
        projects: '/*.UnitTests.csproj'
        arguments: '--configuration $(BuildConfiguration)'

- stage: DeployToDev
  jobs:
  - deployment: DeployToDev
    environment: 'Dev'
    strategy:
      runOnce:
        deploy:
          steps:
          - task: KubernetesManifest@1
            displayName: 'Deploy to Dev AKS'
            # ... deployment details ...
          - task: CmdLine@2
            displayName: 'Run E2E Tests on Dev Environment'
            inputs:
              script: 'npm run¬†e2e-tests¬†--

Good question üëç ‚Äî updating **database schemas** is one of the **most sensitive** operations in production because it directly impacts applications and data.
Let‚Äôs break it down as you‚Äôd do in a **real-time enterprise (AT\&T / FAI / CDK / LTIMindtree)** environment.

---

# üîπ 1. What is a Database Schema Update?

A **schema update** means changing the structure of the DB, not just the data:

* Adding/dropping tables
* Modifying columns (datatype, size, nullability)
* Adding/removing indexes, constraints, keys
* Adding new stored procedures, triggers, views

---

# üîπ 2. Real-Time Process (How Companies Do It)

‚úÖ **Step 1: Version Control (DevOps practice)**

* Database scripts are kept in Git repo (just like app code).
* Example repo structure:

  ```
  db/
   ‚îú‚îÄ‚îÄ migrations/
   ‚îÇ   ‚îú‚îÄ‚îÄ 001_init.sql
   ‚îÇ   ‚îú‚îÄ‚îÄ 002_add_customer_table.sql
   ‚îÇ   ‚îú‚îÄ‚îÄ 003_update_orders_column.sql
   ‚îÇ
   ‚îî‚îÄ‚îÄ rollback/
       ‚îú‚îÄ‚îÄ 002_drop_customer_table.sql
       ‚îú‚îÄ‚îÄ 003_revert_orders_column.sql
  ```

‚úÖ **Step 2: Development & Testing**

* Devs write schema migration scripts.
* Tested on **Dev DB** first (with sample data).

‚úÖ **Step 3: Automated CI/CD Pipeline**

* **CI:** Run DB migrations in a local test DB (SQL Server / Azure SQL).
* **CD:**

  * Deploy infra (Terraform ‚Üí Azure SQL, Cosmos, etc.)
  * Run **migration tool** (Flyway, Liquibase, EF Core Migrations).
  * Verify schema version after migration.

‚úÖ **Step 4: Approvals & Production Deployment**

* Schema changes require **manual approval** (DBA + App Owner).
* Run in **maintenance window** if downtime required.
* Always take a **backup** before running schema updates.

  ```bash
  az sql db export --admin-user sqladmin --admin-password <pwd> \
    --name appdb --resource-group rg-app --server sqlserver01 \
    --storage-key <key> --storage-uri <blob_sas_url>
  ```

---

# üîπ 3. Tools Used in Real Time

* **Azure SQL Migration tools** ‚Üí built-in schema compare in Azure Data Studio.
* **Flyway** (most common in enterprises) ‚Üí versioned migration scripts.
* **Liquibase** ‚Üí declarative XML/JSON migrations.
* **Entity Framework Core Migrations** (for .NET apps).
* **Terraform** ‚Üí can create DBs but NOT good for schema migrations.

---

# üîπ 4. Example: Schema Migration with Flyway (SQL)

**Migration Script:** `V2__add_email_to_users.sql`

```sql
ALTER TABLE Users ADD Email VARCHAR(255) NULL;
```

**Pipeline Task (Azure DevOps YAML):**

```yaml
- task: Flyway@0
  inputs:
    url: 'jdbc:sqlserver://$(SQL_SERVER).database.windows.net:1433;databaseName=$(SQL_DB)'
    user: '$(SQL_USER)'
    password: '$(SQL_PASSWORD)'
    schemas: 'dbo'
    locations: 'filesystem:$(Build.SourcesDirectory)/db/migrations'
    sqlMigrationSuffixes: '.sql'
    command: 'migrate'
```

---

# üîπ 5. Troubleshooting Schema Updates

* **Error: ‚ÄúColumn already exists‚Äù** ‚Üí script not idempotent ‚Üí fix with `IF NOT EXISTS`.

  ```sql
  IF NOT EXISTS (SELECT * FROM sys.columns 
                 WHERE Name = 'Email' AND Object_ID = Object_ID('Users'))
  ALTER TABLE Users ADD Email VARCHAR(255);
  ```

* **App crashes after migration** ‚Üí caused by missing rollback.

  * Always write rollback script:

    ```sql
    ALTER TABLE Users DROP COLUMN Email;
    ```

* **Performance issue after migration** ‚Üí forgot to add indexes after schema change.

  ```sql
  CREATE INDEX IX_Users_Email ON Users (Email);
  ```

---

# üîπ 6. Real Example (FAI / CDK / LTIMindtree)

* **FAI (Insurance DBs)** ‚Üí use Flyway + ServiceNow approval for schema updates.
* **CDK Global (Automotive SaaS)** ‚Üí EF Core migrations built into pipelines, schema validated in QA before prod.
* **LTIMindtree** ‚Üí hybrid (Terraform for infra + Flyway for DB migrations).
* **AT\&T (Telco)** ‚Üí strict change approvals; DBAs manage production migrations, DevOps pipelines run non-prod migrations.

---

‚úÖ **Golden Rule in Real Time:**
Schema migrations must be **repeatable, versioned, tested, and reversible**.
CI/CD pipelines automate them up to **staging**, but **production always requires approval + backup**.

---

üëâ Do you want me to **write you a full Azure DevOps pipeline YAML** that includes both **application deployment + DB schema migration step with rollback** (production-grade)?
            
